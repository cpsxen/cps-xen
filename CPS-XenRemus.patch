diff -urBN xen/tools/libxc/include/xenctrl.h xen-4.10-rc5/tools/libxc/include/xenctrl.h
--- xen/tools/libxc/include/xenctrl.h	2017-12-15 14:52:52.681135000 +0100
+++ xen-4.10-rc5/tools/libxc/include/xenctrl.h	2017-11-21 17:10:01.851106000 +0100
@@ -1000,6 +1000,23 @@
     uint32_t cpupool_id,
     struct xen_sysctl_arinc653_schedule *schedule);
 
+
+int xc_sched_fp_domain_set(xc_interface *xch,
+                               uint32_t domid,
+                               struct xen_domctl_sched_fp *sdom);
+
+int xc_sched_fp_domain_get(xc_interface *xch,
+                               uint32_t domid,
+                               struct xen_domctl_sched_fp *sdom);
+
+int xc_sched_fp_schedule_set(xc_interface *xch, uint32_t poolid,
+				struct xen_sysctl_fp_schedule *schedule);
+
+int xc_sched_fp_schedule_get(xc_interface *xch, uint32_t poolid,
+				struct xen_sysctl_fp_schedule *schedule);
+int xc_sched_fp_get_wcload_on_cpu(xc_interface *xch,
+                                uint32_t cpu, struct xen_sysctl_fp_schedule *schedule);
+
 /**
  * This function sends a trigger to a domain.
  *
diff -urBN xen/tools/libxc/xc_fp.c xen-4.10-rc5/tools/libxc/xc_fp.c
--- xen/tools/libxc/xc_fp.c	1970-01-01 01:00:00.000000000 +0100
+++ xen-4.10-rc5/tools/libxc/xc_fp.c	2017-11-21 17:10:01.851106000 +0100
@@ -0,0 +1,147 @@
+/****************************************************************************
+ * (C) 2014 - Boguslaw Jablkowski, Michael Müller - Technische Universität Dortmund
+ ****************************************************************************
+ *
+ *        File: xc_fp.c
+ *
+ * Description: XC Interface to the fixed priority scheduler
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation;
+ * version 2.1 of the License.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+
+#include "xc_private.h"
+
+int
+xc_sched_fp_domain_set(
+    xc_interface *xch,
+    uint32_t domid,
+    struct xen_domctl_sched_fp *sdom)
+{
+    DECLARE_DOMCTL;
+
+    domctl.cmd = XEN_DOMCTL_scheduler_op;
+    domctl.domain = (domid_t) domid;
+    domctl.u.scheduler_op.sched_id = XEN_SCHEDULER_FP;
+    domctl.u.scheduler_op.cmd = XEN_DOMCTL_SCHEDOP_putinfo;
+    domctl.u.scheduler_op.u.fp = *sdom;
+
+    return do_domctl(xch, &domctl);
+}
+
+int
+xc_sched_fp_domain_get(
+    xc_interface *xch,
+    uint32_t domid,
+    struct xen_domctl_sched_fp *sdom)
+{
+    DECLARE_DOMCTL;
+    int err;
+
+    domctl.cmd = XEN_DOMCTL_scheduler_op;
+    domctl.domain = (domid_t) domid;
+    domctl.u.scheduler_op.sched_id = XEN_SCHEDULER_FP;
+    domctl.u.scheduler_op.cmd = XEN_DOMCTL_SCHEDOP_getinfo;
+
+    err = do_domctl(xch, &domctl);
+    if ( err == 0 )
+        *sdom = domctl.u.scheduler_op.u.fp;
+
+    return err;
+}
+
+int
+xc_sched_fp_schedule_set(
+    xc_interface *xch, uint32_t poolid,
+    struct xen_sysctl_fp_schedule *schedule)
+{
+    int rc;
+    DECLARE_SYSCTL;
+    DECLARE_HYPERCALL_BOUNCE(
+        schedule,
+        sizeof(*schedule),
+        XC_HYPERCALL_BUFFER_BOUNCE_IN);
+    
+    if ( xc_hypercall_bounce_pre(xch, schedule) )
+        return -1;
+
+    sysctl.cmd = XEN_SYSCTL_scheduler_op;
+    sysctl.u.scheduler_op.cpupool_id = poolid;
+    sysctl.u.scheduler_op.sched_id = XEN_SCHEDULER_FP;
+    sysctl.u.scheduler_op.cmd = XEN_SYSCTL_SCHEDOP_putinfo;
+    set_xen_guest_handle(sysctl.u.scheduler_op.u.sched_fp.schedule, schedule);
+    
+    rc = do_sysctl(xch, &sysctl);
+    xc_hypercall_bounce_post(xch, schedule);
+    
+    return rc;
+}
+
+int
+xc_sched_fp_schedule_get(
+    xc_interface *xch,
+    uint32_t poolid,
+    struct xen_sysctl_fp_schedule *schedule)
+{
+    int rc;
+    DECLARE_SYSCTL;
+    DECLARE_HYPERCALL_BOUNCE(
+        schedule,
+        sizeof(*schedule),
+        XC_HYPERCALL_BUFFER_BOUNCE_OUT);
+
+    if ( xc_hypercall_bounce_pre(xch, schedule) )
+        return -1;
+
+    sysctl.cmd = XEN_SYSCTL_scheduler_op;
+    sysctl.u.scheduler_op.cpupool_id = poolid;
+    sysctl.u.scheduler_op.sched_id = XEN_SCHEDULER_FP;
+    sysctl.u.scheduler_op.cmd = XEN_SYSCTL_SCHEDOP_getinfo;
+    set_xen_guest_handle(sysctl.u.scheduler_op.u.sched_fp.schedule,
+            schedule);
+
+    rc = do_sysctl(xch, &sysctl);
+
+    xc_hypercall_bounce_post(xch, schedule);
+    
+    return rc;
+}
+
+int xc_sched_fp_get_wcload_on_cpu(
+    xc_interface *xch, uint32_t cpu, struct xen_sysctl_fp_schedule *schedule)
+{
+    int rc;
+    DECLARE_SYSCTL;
+    DECLARE_HYPERCALL_BOUNCE(
+        schedule,
+        sizeof(*schedule),
+        XC_HYPERCALL_BUFFER_BOUNCE_OUT);
+
+    if ( xc_hypercall_bounce_pre(xch, schedule) ) {
+        return -1;
+    }
+
+    sysctl.cmd = XEN_SYSCTL_scheduler_op;
+    sysctl.u.scheduler_op.cpupool_id = 0;
+    sysctl.u.scheduler_op.cpu = cpu;
+    sysctl.u.scheduler_op.sched_id = XEN_SCHEDULER_FP;
+    sysctl.u.scheduler_op.cmd = XEN_SYSCTL_SCHEDOP_getinfo;
+    set_xen_guest_handle(sysctl.u.scheduler_op.u.sched_fp.schedule,
+        schedule);
+
+    rc = do_sysctl(xch, &sysctl);
+    xc_hypercall_bounce_post(xch, schedule);
+
+    return rc;
+}
diff -urBN xen/tools/libxc/xc_private.c xen-4.10-rc5/tools/libxc/xc_private.c
--- xen/tools/libxc/xc_private.c	2017-12-15 14:52:52.689135000 +0100
+++ xen-4.10-rc5/tools/libxc/xc_private.c	2017-11-21 17:10:01.851106000 +0100
@@ -640,9 +640,11 @@
 
     while ( offset < size )
     {
+
         len = read(fd, (char *)data + offset, size - offset);
-        if ( (len == -1) && (errno == EINTR) )
+        if ( (len == -1) && (errno == EINTR) ) {
             continue;
+        }
         if ( len == 0 )
             errno = 0;
         if ( len <= 0 )
diff -urBN xen/tools/libxc/xc_sr_restore.c xen-4.10-rc5/tools/libxc/xc_sr_restore.c
--- xen/tools/libxc/xc_sr_restore.c	2017-12-15 14:52:52.689135000 +0100
+++ xen-4.10-rc5/tools/libxc/xc_sr_restore.c	2017-12-08 17:35:28.897135000 +0100
@@ -2,6 +2,7 @@
 
 #include <assert.h>
 
+#include <sys/time.h>
 #include "xc_sr_common.h"
 
 /*
@@ -735,6 +736,8 @@
     xc_interface *xch = ctx->xch;
     struct xc_sr_record rec;
     int rc, saved_rc = 0, saved_errno = 0;
+    FILE *cpsr_log;
+    struct timeval tv;
 
     IPRINTF("Restoring domain");
 
@@ -799,6 +802,11 @@
      * With Remus, if we reach here, there must be some error on primary,
      * failover from the last checkpoint state.
      */
+    cpsr_log = fopen("/tmp/remus_trace.log", "a");
+    gettimeofday(&tv, NULL);
+    fprintf(cpsr_log, "Time broken channel is recognized in xc: %lu\n", tv.tv_sec*1000000+tv.tv_usec);
+    fclose(cpsr_log);
+
     rc = ctx->restore.ops.stream_complete(ctx);
     if ( rc )
         goto err;
diff -urBN xen/tools/libxc/Makefile xen-4.10-rc5/tools/libxc/Makefile
--- xen/tools/libxc/Makefile	2017-12-15 14:52:52.681135000 +0100
+++ xen-4.10-rc5/tools/libxc/Makefile	2017-11-21 17:10:01.847106000 +0100
@@ -26,6 +26,7 @@
 CTRL_SRCS-y       += xc_csched2.c
 CTRL_SRCS-y       += xc_arinc653.c
 CTRL_SRCS-y       += xc_rt.c
+CTRL_SRCS-y       += xc_fp.c
 CTRL_SRCS-y       += xc_tbuf.c
 CTRL_SRCS-y       += xc_pm.c
 CTRL_SRCS-y       += xc_cpu_hotplug.c
diff -urBN xen/tools/libxl/Makefile xen-4.10-rc5/tools/libxl/Makefile
--- xen/tools/libxl/Makefile	2017-12-15 14:52:52.693135000 +0100
+++ xen-4.10-rc5/tools/libxl/Makefile	2017-11-23 15:35:07.119106000 +0100
@@ -181,7 +181,7 @@
 
 $(TEST_PROG_OBJS) _libxl.api-for-check: CFLAGS += $(CFLAGS_libxentoollog) $(CFLAGS_libxentoolcore)
 
-CLIENTS = testidl libxl-save-helper
+CLIENTS = testidl libxl-save-helper cpsremus_heartbeat_send cpsremus_heartbeat_rcv
 
 libxl_dom.o: CFLAGS += -I$(XEN_ROOT)/tools  # include libacpi/x86.h
 libxl_x86_acpi.o: CFLAGS += -I$(XEN_ROOT)/tools
@@ -189,6 +189,9 @@
 SAVE_HELPER_OBJS = libxl_save_helper.o _libxl_save_msgs_helper.o
 $(SAVE_HELPER_OBJS): CFLAGS += $(CFLAGS_libxenctrl) $(CFLAGS_libxenevtchn)
 
+CPSREMUS_HB_OBJS = cpsremus_heartbeat_rcv.o
+$(CPSREMUS_HB_OBJS): CFLAGS += $(CFLAGS_libxentoollog) $(CFLAGS_XL) -include $(XEN_ROOT)/tools/config.h
+
 PKG_CONFIG = xenlight.pc xlutil.pc
 PKG_CONFIG_VERSION := $(MAJOR).$(MINOR)
 
@@ -307,6 +310,10 @@
 libxl-save-helper: $(SAVE_HELPER_OBJS) libxenlight.so
 	$(CC) $(LDFLAGS) -o $@ $(SAVE_HELPER_OBJS) $(LDLIBS_libxentoollog) $(LDLIBS_libxenctrl) $(LDLIBS_libxenguest) $(LDLIBS_libxentoolcore) $(APPEND_LDFLAGS)
 
+cpsremus_heartbeat_rcv: $(CPSREMUS_HB_OBJS) libxenlight.so
+	$(CC) $(LDFLAGS) -o $@ $(CPSREMUS_HB_OBJS) libxlutil.so $(LDLIBS_libxenlight) $(LDLIBS_libxentoollog) -lyajl $(APPEND_LDFLAGS)
+
+
 testidl: testidl.o libxlutil.so libxenlight.so
 	$(CC) $(LDFLAGS) -o $@ testidl.o libxlutil.so $(LDLIBS_libxenlight) $(LDLIBS_libxentoollog) $(LDLIBS_libxentoolcore) $(APPEND_LDFLAGS)
 
@@ -316,6 +323,9 @@
 	$(INSTALL_DIR) $(DESTDIR)$(includedir)
 	$(INSTALL_DIR) $(DESTDIR)$(LIBEXEC_BIN)
 	$(INSTALL_PROG) libxl-save-helper $(DESTDIR)$(LIBEXEC_BIN)
+	$(INSTALL_PROG) cpsremus_heartbeat_rcv $(DESTDIR)$(LIBEXEC_BIN)
+	$(INSTALL_PROG) cpsremus_heartbeat_send $(DESTDIR)$(LIBEXEC_BIN)
+	$(INSTALL_PROG) heartbeat_launcher $(DESTDIR)$(LIBEXEC_BIN)
 	$(INSTALL_SHLIB) libxenlight.so.$(MAJOR).$(MINOR) $(DESTDIR)$(libdir)
 	$(SYMLINK_SHLIB) libxenlight.so.$(MAJOR).$(MINOR) $(DESTDIR)$(libdir)/libxenlight.so.$(MAJOR)
 	$(SYMLINK_SHLIB) libxenlight.so.$(MAJOR) $(DESTDIR)$(libdir)/libxenlight.so
diff -urBN xen/tools/libxl/cpsremus_heartbeat_rcv.c xen-4.10-rc5/tools/libxl/cpsremus_heartbeat_rcv.c
--- xen/tools/libxl/cpsremus_heartbeat_rcv.c	1970-01-01 01:00:00.000000000 +0100
+++ xen-4.10-rc5/tools/libxl/cpsremus_heartbeat_rcv.c	2017-12-07 15:54:51.472820000 +0100
@@ -0,0 +1,127 @@
+#include <stdio.h>
+#include <unistd.h>
+#include <signal.h>
+#include <sys/select.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <sys/time.h>
+#include <stdlib.h>
+#include <errno.h>
+#include <fcntl.h>
+#include "libxl.h"
+#include "libxl_utils.h"
+#include "libxlutil.h"
+
+#define UNITS 1000 // unit is milliseconds
+
+libxl_ctx *ctx;
+
+char** parse_pids(char *pids);
+
+char** parse_pids(char *pids) {
+    char **ret;
+    
+    fprintf(stderr,"Entered parsing function\n");
+    ret = (char**)malloc(sizeof(char*)*2);
+    ret[0] = pids;  
+
+    for (;*pids != '#' && *pids != '\0'; pids++)
+      fprintf(stderr, "%c",*pids);
+    *pids = '\0';
+    ret[1] = pids+1;
+    
+    fprintf(stderr,"Leaving parsing function\n");
+    return ret;
+} 
+
+int main(int argc, char** argv)
+{
+    char hb;
+
+    pid_t remus_pid;
+    int domid;
+    fd_set rdfs;
+    struct timeval tv;
+    int ret;
+    int bytes_read = 0;
+    int i = 0;
+    struct stat st_info;
+    char pids[13];
+    int fifo_fd;
+    char **pidss;
+    int timeout;
+    libxl_device_nic *nics;
+    int nb;
+
+    if (argc != 2) {
+        fprintf(stderr, "Not enough arguments");
+        return -1;
+    }
+
+    timeout = (pid_t)atoi(argv[1]);   
+
+    if (stat("/tmp/cpsremus_fifo", &st_info)) {
+        if (errno == ENOENT) {
+            mkfifo("/tmp/cpsremus_fifo", 0666);
+        }
+    }
+   
+    fprintf(stderr, "Opening fifo for reading.\n"); 
+    fifo_fd = open("/tmp/cpsremus_fifo", O_RDONLY);
+    
+    fprintf(stderr, "Reading pids from fifo\n");
+    bytes_read = read(fifo_fd, pids, 12);
+
+    close(fifo_fd);
+
+    fprintf(stderr, "Content of pids is: %s after %i bytes read.\n", pids, bytes_read);
+    
+    pidss = parse_pids(pids);
+    fprintf(stderr, "pidss[0] = %s\n",pidss[0]);
+    fprintf(stderr, "pidss[1] = %s\n",pidss[1]);
+    fflush(stderr);
+    remus_pid = atoi(pidss[0]);
+    domid = atoi(pidss[1]);
+
+    free(pidss);
+    fprintf(stderr, "Pid of save-helper is %i\n",remus_pid);
+
+    do {
+        FD_ZERO(&rdfs);
+        FD_SET(0, &rdfs);
+
+        tv.tv_sec = timeout/UNITS;
+        tv.tv_usec = timeout%UNITS*1000;
+        ret = select(1, &rdfs, NULL, NULL, &tv);
+        
+        if (!ret) {
+	    gettimeofday(&tv, NULL);
+	    fprintf(stderr, "Timestamp failover recognized: %lu\n", tv.tv_sec*1000000+tv.tv_usec);
+            fprintf(stderr, "No heartbeat from primary within %d milliseconds. Failover.\n", timeout);
+            fprintf(stderr, "Killing ssh process with pid: %d\n", remus_pid);
+	        kill(remus_pid, SIGTERM);
+            return 1;
+        }
+
+        printf("%i",i);
+        if (FD_ISSET(0, &rdfs)) {
+            bytes_read = read(0, &hb, 1);
+            printf("%c",hb);
+        }
+        i++;
+    } while ( bytes_read > 0 );
+    gettimeofday(&tv, NULL);
+    fprintf(stderr, "Timestamp failover recognized: %lu\n", tv.tv_sec*1000000+tv.tv_usec);
+    fprintf(stderr, "No heartbeat from primary within %d seconds. Failover.\n", timeout);
+    fprintf(stderr, "Killing ssh process with pid: %d\n", remus_pid);
+    kill(remus_pid, SIGTERM);
+
+    /* Send a gratuitous arp for instant change of mac addresses and saved switch ports. */
+    nics = libxl_device_nic_list(ctx, domid, &nb);
+    if (nics && nb) {
+        for (i = 0; i < nb; ++i);
+            libxl_device_nic_send_gratuitous_arp(ctx, &nics[i]);
+    }
+
+    return 1;
+}   
diff -urBN xen/tools/libxl/cpsremus_heartbeat_send.c xen-4.10-rc5/tools/libxl/cpsremus_heartbeat_send.c
--- xen/tools/libxl/cpsremus_heartbeat_send.c	1970-01-01 01:00:00.000000000 +0100
+++ xen-4.10-rc5/tools/libxl/cpsremus_heartbeat_send.c	2017-11-23 15:20:35.459106000 +0100
@@ -0,0 +1,27 @@
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+
+#define PERCENTAGE 33 // percentage of timeout to be used as period for heartbeat signal
+#define UNITS 1000 // use units of 1000 microseconds for period
+
+int main(int argc, char *argv[]) {
+
+    unsigned int timeout;
+    unsigned int period;
+
+    if (argc != 2) {
+        fprintf(stderr, "error. sender must be started with an interval specification.\n");
+        return -1;
+    }
+
+    timeout = atoi(argv[1]);
+    period = timeout*PERCENTAGE/100;
+
+    while(1) {
+        fprintf(stdout, "h\n");
+        fflush(stdout);
+        usleep(period*UNITS);
+    }
+    return -1;
+}
diff -urBN xen/tools/libxl/heartbeat_launcher xen-4.10-rc5/tools/libxl/heartbeat_launcher
--- xen/tools/libxl/heartbeat_launcher	1970-01-01 01:00:00.000000000 +0100
+++ xen-4.10-rc5/tools/libxl/heartbeat_launcher	2017-11-23 15:20:44.755106000 +0100
@@ -0,0 +1,3 @@
+#!/bin/sh
+
+cpsremus_heartbeat_send $2 | ssh $1 "cpsremus_heartbeat_rcv $2 &> /var/log/cpsremus.log" 
diff -urBN xen/tools/libxl/libxl.h xen-4.10-rc5/tools/libxl/libxl.h
--- xen/tools/libxl/libxl.h	2017-12-15 14:52:52.693135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl.h	2017-11-23 14:34:16.351106000 +0100
@@ -1893,6 +1893,7 @@
 int libxl_device_nic_getinfo(libxl_ctx *ctx, uint32_t domid,
                              libxl_device_nic *nic, libxl_nicinfo *nicinfo)
                              LIBXL_EXTERNAL_CALLERS_ONLY;
+int libxl_device_nic_send_gratuitous_arp(libxl_ctx *ctx, libxl_device_nic *nic);
 
 /*
  * Virtual Channels
@@ -2141,16 +2142,29 @@
                                    libxl_sched_credit2_params *scinfo);
 int libxl_sched_credit2_params_set(libxl_ctx *ctx, uint32_t poolid,
                                    libxl_sched_credit2_params *scinfo);
+int libxl_sched_fp_schedule_get(libxl_ctx *ctx, uint32_t poolid, 
+                                  libxl_sched_fp_params *scinfo);
+int libxl_sched_fp_schedule_set(libxl_ctx *ctx, uint32_t poolid,
+                                  libxl_sched_fp_params *scinfo);
+int libxl_sched_fp_get_wcload_on_cpu(libxl_ctx *ctx, int cpu, libxl_sched_fp_params *scinfo);
 
 /* Scheduler Per-domain parameters */
 
 #define LIBXL_DOMAIN_SCHED_PARAM_WEIGHT_DEFAULT    -1
 #define LIBXL_DOMAIN_SCHED_PARAM_CAP_DEFAULT       -1
-#define LIBXL_DOMAIN_SCHED_PARAM_PERIOD_DEFAULT    -1
-#define LIBXL_DOMAIN_SCHED_PARAM_SLICE_DEFAULT     -1
+#define LIBXL_DOMAIN_SCHED_PARAM_PERIOD_DEFAULT    100
+#define LIBXL_DOMAIN_SCHED_PARAM_SLICE_DEFAULT     100
 #define LIBXL_DOMAIN_SCHED_PARAM_LATENCY_DEFAULT   -1
 #define LIBXL_DOMAIN_SCHED_PARAM_EXTRATIME_DEFAULT -1
 #define LIBXL_DOMAIN_SCHED_PARAM_BUDGET_DEFAULT    -1
+#define LIBXL_DOMAIN_SCHED_PARAM_DEADLINE_DEFAULT  100
+#define LIBXL_DOMAIN_SCHED_PARAM_PRIORITY_DEFAULT  999
+#define LIBXL_DOMAIN_SCHED_PARAM_PRIORITY_MAX     1000
+
+/* RM/DM/FP-Scheduler stratgegies */
+#define LIBXL_SCHED_FP_STRAT_RM 0
+#define LIBXL_SCHED_FP_STRAT_DM 1
+#define LIBXL_SCHED_FP_STRAT_FP 2
 
 /* Per-VCPU parameters */
 #define LIBXL_SCHED_PARAM_VCPU_INDEX_DEFAULT   -1
diff -urBN xen/tools/libxl/libxl_create.c xen-4.10-rc5/tools/libxl/libxl_create.c
--- xen/tools/libxl/libxl_create.c	2017-12-15 14:52:52.697135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_create.c	2017-12-08 15:46:29.012924000 +0100
@@ -781,6 +781,12 @@
                                    libxl__domain_create_state *dcs,
                                    int ret);
 
+static void nic_create_cb(libxl__egc *egc, libxl__multidev *aodevs, int ret) { 
+	/*FILE *log = fopen("/tmp/remus_trace.log","a");
+	fprintf(log, "In %s\n", __func__);
+	fclose(log);*/
+return; }
+
 /* Our own function to clean up and call the user's callback.
  * The final call in the sequence. */
 static void domcreate_complete(libxl__egc *egc,
@@ -982,6 +988,12 @@
 
     if (restore_fd >= 0 || dcs->domid_soft_reset != INVALID_DOMID) {
         LOGD(DEBUG, domid, "restoring, not running bootloader");
+        if (d_config->num_nics > 0) {
+            libxl__multidev_begin(ao, &dcs->multidev);
+            dcs->multidev.callback = nic_create_cb;
+            libxl__add_nics(egc, ao, domid, d_config, &dcs->multidev);
+            libxl__multidev_prepared(egc, &dcs->multidev, 0);
+        }
         domcreate_bootloader_done(egc, &dcs->bl, 0);
     } else  {
         LOGD(DEBUG, domid, "running bootloader");
@@ -1500,6 +1512,8 @@
     dt = device_type_tbl[dcs->device_type_idx];
     if (dt) {
         if (*libxl__device_type_get_num(dt, d_config) > 0 && !dt->skip_attach) {
+            if (dt == &libxl__nic_devtype && dcs->restore_fd >= 0)
+                goto skip_attach;
             /* Attach devices */
             libxl__multidev_begin(ao, &dcs->multidev);
             dcs->multidev.callback = domcreate_attach_devices;
@@ -1508,6 +1522,7 @@
             return;
         }
 
+skip_attach:
         domcreate_attach_devices(egc, &dcs->multidev, 0);
         return;
     }
diff -urBN xen/tools/libxl/libxl_dom_suspend.c xen-4.10-rc5/tools/libxl/libxl_dom_suspend.c
--- xen/tools/libxl/libxl_dom_suspend.c	2017-12-15 14:52:52.697135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_dom_suspend.c	2017-11-23 14:35:57.495106000 +0100
@@ -33,8 +33,11 @@
     libxl__xswait_init(&dsps->pvcontrol);
     libxl__ev_evtchn_init(&dsps->guest_evtchn);
     libxl__ev_xswatch_init(&dsps->guest_watch);
+    libxl__ev_xswatch_init(&dsps->cpsremus_watch);
     libxl__ev_time_init(&dsps->guest_timeout);
 
+    libxl__ev_xswatch_deregister(gc, &dsps->cpsremus_watch);
+
     if (type == LIBXL_DOMAIN_TYPE_INVALID) goto out;
     dsps->type = type;
 
diff -urBN xen/tools/libxl/libxl_domain.c xen-4.10-rc5/tools/libxl/libxl_domain.c
--- xen/tools/libxl/libxl_domain.c	2017-12-15 14:52:52.697135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_domain.c	2017-11-23 14:32:45.395106000 +0100
@@ -412,6 +412,8 @@
                              !libxl_defbool_val(info->colo));
     libxl_defbool_setdefault(&info->netbuf, true);
     libxl_defbool_setdefault(&info->diskbuf, true);
+    libxl_defbool_setdefault(&info->event_driven, false);
+    libxl_defbool_setdefault(&info->polling, false);
 
     if (libxl_defbool_val(info->colo) &&
         libxl_defbool_val(info->compression)) {
@@ -442,6 +444,32 @@
     dss->live = 1;
     dss->debug = 0;
     dss->remus = info;
+    dss->statepath = NULL;
+
+    if (libxl_defbool_val(info->event_driven)) {
+        /* Check if DomU has support for event-driven checkpointing */
+        xs_transaction_t t = 0;
+        int trc = 0;
+        
+        trc = libxl__xs_transaction_start(gc, &t);
+        char *dompath = libxl__xs_get_dompath(gc, dss->domid);
+        char *statepath = libxl__sprintf(gc, "%s/data/ha", dompath);
+
+        if (trc) {
+            LOG(ERROR, "Remus: Failed to check DomU support");
+            goto out;
+        }
+
+        if (!libxl__xs_read(gc, t, statepath)) {
+            libxl__xs_transaction_abort(gc, &t);
+            LOG(ERROR, "CPS-Remus: Event-driven checkpointing not supported by domain. Aborting.");
+            goto out;
+        }
+        
+        libxl__xs_transaction_abort(gc, &t);
+        dss->statepath = statepath;
+    }
+
     if (libxl_defbool_val(info->colo))
         dss->checkpointed_stream = LIBXL_CHECKPOINTED_STREAM_COLO;
     else
@@ -460,6 +488,24 @@
     return AO_CREATE_FAIL(rc);
 }
 
+int libxl_device_nic_send_gratuitous_arp(libxl_ctx *ctx, libxl_device_nic *nic)
+{
+    GC_INIT(ctx);
+    char mac[18];
+    int rc;
+    char *cmd;
+
+   sprintf(mac, LIBXL_MAC_FMT, LIBXL_MAC_BYTES(nic->mac)); 
+    cmd = libxl__sprintf(gc, "send_arp %s %s %s ff:ff:ff:ff:ff:ff %s %s ff:ff:ff:ff:ff:ff request", nic->ip, mac, nic->ip, nic->bridge, mac);  
+    fprintf(stderr, "Updating arp for ip %s and mac %s on bridge %s\n", nic->ip, mac, nic->bridge);
+    rc = system(cmd);
+    if (rc)
+        fprintf(stderr, "Unable to send arp packet\n");
+
+    GC_FREE;
+    return rc;
+}
+
 static void remus_failover_cb(libxl__egc *egc,
                               libxl__domain_save_state *dss, int rc)
 {
diff -urBN xen/tools/libxl/libxl_internal.h xen-4.10-rc5/tools/libxl/libxl_internal.h
--- xen/tools/libxl/libxl_internal.h	2017-12-15 14:52:52.701135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_internal.h	2017-12-08 15:26:04.352924000 +0100
@@ -3262,6 +3262,7 @@
 
     libxl__xswait_state pvcontrol;
     libxl__ev_xswatch guest_watch;
+    libxl__ev_xswatch cpsremus_watch;
     libxl__ev_time guest_timeout;
 
     const char *dm_savefile;
@@ -3284,8 +3285,11 @@
     libxl_domain_type type;
     int live;
     int debug;
+    char *statepath;
     int checkpointed_stream;
     const libxl_domain_remus_info *remus;
+    libxl__ev_xswatch guest_watch;
+    libxl__ev_xswatch cpsremus_watch;
     /* private */
     int rc;
     int hvm;
diff -urBN xen/tools/libxl/libxl_remus.c xen-4.10-rc5/tools/libxl/libxl_remus.c
--- xen/tools/libxl/libxl_remus.c	2017-12-15 14:52:52.721135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_remus.c	2017-11-23 15:18:59.691106000 +0100
@@ -69,6 +69,9 @@
 static void libxl__remus_domain_suspend_callback(void *data);
 static void libxl__remus_domain_resume_callback(void *data);
 static void libxl__remus_domain_save_checkpoint_callback(void *data);
+static void cpsremus_next_checkpoint(libxl__egc *egc, libxl__ev_xswatch *ev, 
+                                     const char *watch_path,
+                                     const char *event_path);
 
 void libxl__remus_setup(libxl__egc *egc, libxl__remus_state *rs)
 {
@@ -328,6 +331,7 @@
                                     int rc)
 {
     libxl__domain_save_state *dss = CONTAINER_OF(cds, *dss, cds);
+    static int count = 0;
 
     STATE_AO_GC(dss->ao);
 
@@ -343,11 +347,22 @@
      * interval to checkpoint the guest again. Until then, let the guest
      * continue execution.
      */
+    if (libxl_defbool_val(dss->remus->event_driven)) {
+        LOG(WARN, "Activated CPS-Remus. Statepath for watch is %s\n", dss->statepath);
+        /* Use event-driven checkpointing */
+        LOG(WARN, "Count is: %d", count);
+        if (!count) {
+            LOG(WARN, ">>>>> Registering xswatch on statepath: %s\n", dss->statepath);
+            libxl__ev_xswatch_register(gc, &dss->cpsremus_watch, cpsremus_next_checkpoint, dss->statepath);
+            count++;
 
-    /* Set checkpoint interval timeout */
-    rc = libxl__ev_time_register_rel(ao, &dss->rs.checkpoint_timeout,
-                                     remus_next_checkpoint,
-                                     dss->rs.interval);
+        }
+    } else {
+        /* Set checkpoint interval timeout */
+        rc = libxl__ev_time_register_rel(ao, &dss->rs.checkpoint_timeout,
+                                         remus_next_checkpoint,
+                                         dss->rs.interval);
+    }
 
     if (rc)
         goto out;
@@ -379,9 +394,44 @@
     if (rc)
         dss->rc = rc;
 
-    libxl__xc_domain_saverestore_async_callback_done(egc, &dss->sws.shs, !rc);
+    if (libxl_defbool_val(dss->remus->polling)) {
+        xs_transaction_t t = 0;
+        if (!libxl__xs_transaction_start(gc, &t)) {
+            int state = atoi(libxl__xs_read(gc, t, dss->statepath));
+            if (state > 0) {
+                //libxl__xs_write(gc, t, dss->statepath, "%d", 0);
+                libxl__xc_domain_saverestore_async_callback_done(egc, &dss->sws.shs, !rc);
+            } else {
+                libxl__ev_time_register_rel(ao, &dss->rs.checkpoint_timeout,
+                                         remus_next_checkpoint,
+                                         dss->rs.interval);
+            }
+            libxl__xs_transaction_commit(gc, &t);
+        }
+    } else
+        libxl__xc_domain_saverestore_async_callback_done(egc, &dss->sws.shs, !rc);
 }
 
+static void cpsremus_next_checkpoint(libxl__egc *egc, libxl__ev_xswatch *ev,
+                                     const char *watch_path,
+                                     const char *event_path)
+{
+    libxl__domain_save_state *dss =
+                            CONTAINER_OF(ev, *dss, cpsremus_watch);
+
+    STATE_AO_GC(dss->ao);
+
+    /*
+     * Time to checkpoint the guest again. We return 1 to libxc
+     * (xc_domain_save.c). in order to continue executing the infinite loop
+     * (suspend, checkpoint, resume) in xc_domain_save().
+     */
+
+    int rc = 0;
+    dss->rc = 0;
+
+    libxl__xc_domain_saverestore_async_callback_done(egc, &dss->sws.shs, !rc);
+} 
 /*---------------------- remus callbacks (restore) -----------------------*/
 
 /*----- remus asynchronous checkpoint callback -----*/
diff -urBN xen/tools/libxl/libxl_save_callout.c xen-4.10-rc5/tools/libxl/libxl_save_callout.c
--- xen/tools/libxl/libxl_save_callout.c	2017-12-15 14:52:52.721135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_save_callout.c	2017-12-07 16:03:59.508820000 +0100
@@ -12,6 +12,9 @@
  * GNU Lesser General Public License for more details.
  */
 
+#define _GNU_SOURCE
+#include <sys/stat.h>
+
 #include "libxl_osdeps.h"
 
 #include "libxl_internal.h"
@@ -232,6 +235,31 @@
                     args[0], (char**)args, 0);
     }
 
+    pid_t ppid = getppid();
+
+    struct stat stinfo;
+
+    if (stat("/tmp/cpsremus_fifo", &stinfo)) {
+      if (errno == ENOENT) {
+        int ret = mkfifo("/tmp/cpsremus_fifo", 0666);
+        LOGE(WARN, "created fifo with return code %i", ret);
+      }
+    } else {
+     if (!S_ISFIFO(stinfo.st_mode))
+        LOGE(WARN, "cspremus_fifo is not a named pipe.");
+    }
+
+    int fifo_fd = open("/tmp/cpsremus_fifo", O_WRONLY|O_NONBLOCK);
+    char *pid_s; 
+    int pid_s_len = asprintf(&pid_s,"%i#%i",ppid, shs->domid);
+
+    if (write(fifo_fd, (void*)pid_s, pid_s_len+1) != pid_s_len+1) {
+        LOGE(WARN, "unable to write pids to fifo file.");
+   }
+
+    close(fifo_fd);
+
+    LOGE(WARN,"PID of save_helper is %i\n", (int)pid);
     libxl__carefd_close(childs_pipes[0]);
     libxl__carefd_close(childs_pipes[1]);
 
diff -urBN xen/tools/libxl/libxl_save_helper.c xen-4.10-rc5/tools/libxl/libxl_save_helper.c
--- xen/tools/libxl/libxl_save_helper.c	2017-12-15 14:52:52.721135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_save_helper.c	2017-11-23 14:47:51.815106000 +0100
@@ -158,6 +158,17 @@
     errno = esave;
 }
 
+static void remus_failover_handler(int sig)
+{
+    char cmd[30];
+    
+   sprintf(cmd, "trigger_failover.sh %i", (int)getpid());
+
+   if (system(cmd) == -1) {
+        perror("system");
+    }
+}
+
 static void setup_signals(void (*handler)(int))
 {
     struct sigaction sa;
@@ -173,6 +184,12 @@
     r = sigaction(SIGTERM, &sa, 0);
     if (r) fail(errno,"sigaction SIGTERM failed");
 
+    sa.sa_handler = remus_failover_handler;
+    sa.sa_flags = 0;
+    //sigemptyset(&sa.sa_mask);
+    r = sigaction(SIGUSR1, &sa, 0);
+    if (r) fail(errno, "sigaction SIGUSR1 failed");
+
     sigemptyset(&spmask);
     sigaddset(&spmask,SIGTERM);
     r = sigprocmask(SIG_UNBLOCK,&spmask,0);
diff -urBN xen/tools/libxl/libxl_sched.c xen-4.10-rc5/tools/libxl/libxl_sched.c
--- xen/tools/libxl/libxl_sched.c	2017-12-15 14:52:52.721135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_sched.c	2017-11-21 17:33:30.123106000 +0100
@@ -755,6 +755,140 @@
     return 0;
 }
 
+ 
+/* Get domain-parameters for the FP-Scheduler. */
+static int sched_fp_domain_get(libxl__gc *gc, uint32_t domid, libxl_domain_sched_params *scinfo)
+{
+    struct xen_domctl_sched_fp sdom;
+    int rc;
+    
+    rc = xc_sched_fp_domain_get(CTX->xch, domid, &sdom);
+    if (rc != 0) {
+        LIBXL__LOG_ERRNO(CTX, LIBXL__LOG_ERROR, "getting domain sched fp");
+        return ERROR_FAIL;
+    }
+    
+    libxl_domain_sched_params_init(scinfo);
+
+    scinfo->sched = LIBXL_SCHEDULER_FP;
+    scinfo->priority = sdom.priority;
+    scinfo->period = sdom.period / 1000;
+    scinfo->slice = sdom.slice / 1000;
+    scinfo->deadline = sdom.deadline / 1000;
+
+    return 0;
+}
+
+/* Set domain-parameters for the FP-Scheduler. */
+static int sched_fp_domain_set(libxl__gc *gc, uint32_t domid, const libxl_domain_sched_params *scinfo)
+{
+    struct xen_domctl_sched_fp sdom;
+    xc_domaininfo_t domaininfo;
+    int rc;
+
+    rc = xc_domain_getinfolist(CTX->xch, domid, 1, &domaininfo);
+    if (rc <  0) {
+        LIBXL__LOG_ERRNO(CTX, LIBXL__LOG_ERROR, "getting domain info list");
+        return ERROR_FAIL;
+    }
+    if (rc != 1 || domaininfo.domain != domid)
+        return ERROR_INVAL;
+
+    if (scinfo->period < 0) {
+        LIBXL__LOG_ERRNOVAL(CTX, LIBXL__LOG_ERROR, rc, 
+           "Period out of range. Valid values are positive integers.");
+        return ERROR_INVAL;
+    }
+
+    if (scinfo->deadline < 0) {
+        LIBXL__LOG_ERRNOVAL(CTX, LIBXL__LOG_ERROR, rc, 
+            "Deadline out of range. Valid values are positive integers.");
+        return ERROR_INVAL;
+   }
+    
+    if (scinfo->slice < 0) {
+        LIBXL__LOG_ERRNOVAL(CTX, LIBXL__LOG_ERROR, rc, 
+            "Slice out of range. Valid values are positive integers.");
+        return ERROR_INVAL;
+    }
+
+    if (scinfo->priority < 0 || scinfo->priority >= LIBXL_DOMAIN_SCHED_PARAM_PRIORITY_MAX) {
+        LIBXL__LOG_ERRNOVAL(CTX, LIBXL__LOG_ERROR, rc,
+            "Priority out of range. Valid values are between 0 and 999.");
+        return ERROR_INVAL;
+    }
+
+    sdom.priority = scinfo->priority;
+    sdom.slice = scinfo->slice;
+    sdom.period = scinfo->period;
+    sdom.deadline = scinfo->deadline;
+
+    rc = xc_sched_fp_domain_set(CTX->xch, domid, &sdom);
+    if ( rc < 0 ) {
+        LIBXL__LOG_ERRNO(CTX, LIBXL__LOG_ERROR, "setting domain sched credit");
+        return ERROR_FAIL;
+    }
+
+    return 0;
+}
+
+/* Get the currently used scheduling strategy and store it in scinfo. */
+int libxl_sched_fp_schedule_get(libxl_ctx *ctx, uint32_t poolid, libxl_sched_fp_params *scinfo)
+{
+    struct xen_sysctl_fp_schedule schedule;
+    int rc;
+
+    rc = xc_sched_fp_schedule_get(ctx->xch, poolid, &schedule);
+    if (rc != 0) {
+        LIBXL__LOG_ERRNO(ctx, LIBXL__LOG_ERROR, "setting schedule sched fp");
+        return ERROR_FAIL;
+    }
+    
+    scinfo->strategy = schedule.strategy;
+    
+    return 0;
+}
+
+/* Set the new strategy to be used by the FP-Scheduler. */
+int libxl_sched_fp_schedule_set(libxl_ctx *ctx, uint32_t poolid, libxl_sched_fp_params *scinfo)
+{
+    struct xen_sysctl_fp_schedule schedule;
+    int rc;
+    
+    if (scinfo->strategy > 2 || scinfo->strategy < 0) {
+        LIBXL__LOG_ERRNO(ctx, LIBXL__LOG_ERROR, "Unknown strategy. Valid values are 0 for rate-monotonic, 1 for deadline-monotonic or 2 for fixed priority.");
+        return ERROR_INVAL;
+    }
+    
+    schedule.strategy = scinfo->strategy;
+    rc = xc_sched_fp_schedule_set(ctx->xch, poolid, &schedule);
+
+    if (rc != 0) {
+        LIBXL__LOG_ERRNO(ctx, LIBXL__LOG_ERROR, "setting schedule sched fp");
+        return ERROR_FAIL;
+    }
+    
+    return 0;
+}
+
+
+/* Get the hypothetical worst-case load of cpu cpu. */
+int libxl_sched_fp_get_wcload_on_cpu(libxl_ctx *ctx, int cpu, libxl_sched_fp_params *scinfo)
+{
+    struct xen_sysctl_fp_schedule schedule;
+    int rc;
+
+    rc = xc_sched_fp_get_wcload_on_cpu(ctx->xch, cpu, &schedule);
+    if (rc != 0) {
+        LIBXL__LOG_ERRNO(ctx, LIBXL__LOG_ERROR, "getting worst-case load on cpu");
+        return ERROR_FAIL;
+    }
+
+    scinfo->load = schedule.load;
+
+    return 0;
+}
+
 int libxl_domain_sched_params_set(libxl_ctx *ctx, uint32_t domid,
                                   const libxl_domain_sched_params *scinfo)
 {
@@ -785,6 +919,9 @@
     case LIBXL_SCHEDULER_NULL:
         ret=sched_null_domain_set(gc, domid, scinfo);
         break;
+    case LIBXL_SCHEDULER_FP:
+        ret=sched_fp_domain_set(gc, domid, scinfo);
+        break;
     default:
         LOGD(ERROR, domid, "Unknown scheduler");
         ret=ERROR_INVAL;
@@ -892,6 +1029,9 @@
     case LIBXL_SCHEDULER_NULL:
         ret=sched_null_domain_get(gc, domid, scinfo);
         break;
+    case LIBXL_SCHEDULER_FP:
+        ret=sched_fp_domain_get(gc, domid, scinfo);
+        break;
     default:
         LOGD(ERROR, domid, "Unknown scheduler");
         ret=ERROR_INVAL;
diff -urBN xen/tools/libxl/libxl_types.idl xen-4.10-rc5/tools/libxl/libxl_types.idl
--- xen/tools/libxl/libxl_types.idl	2017-12-15 14:52:52.721135000 +0100
+++ xen-4.10-rc5/tools/libxl/libxl_types.idl	2017-11-23 15:15:48.827106000 +0100
@@ -194,6 +194,7 @@
     (7, "arinc653"),
     (8, "rtds"),
     (9, "null"),
+    (10, "fp"),
     ])
 
 # Consistent with SHUTDOWN_* in sched.h (apart from UNKNOWN)
@@ -422,6 +423,8 @@
     ("period",       integer, {'init_val': 'LIBXL_DOMAIN_SCHED_PARAM_PERIOD_DEFAULT'}),
     ("budget",       integer, {'init_val': 'LIBXL_DOMAIN_SCHED_PARAM_BUDGET_DEFAULT'}),
     ("extratime",    integer, {'init_val': 'LIBXL_DOMAIN_SCHED_PARAM_EXTRATIME_DEFAULT'}),
+    ("deadline",     integer, {'init_val': 'LIBXL_DOMAIN_SCHED_PARAM_DEADLINE_DEFAULT'}),
+    ("priority",     integer, {'init_val': 'LIBXL_DOMAIN_SCHED_PARAM_PRIORITY_DEFAULT'}),
 
     # The following three parameters ('slice' and 'latency') are deprecated,
     # and will have no effect if used, since the SEDF scheduler has been removed.
@@ -971,12 +974,18 @@
     ("ratelimit_us", integer),
     ], dispose_fn=None)
 
+libxl_sched_fp_params = Struct("sched_fp_params", [
+    ("strategy", uint8),
+    ("load", uint32),
+    ], dispose_fn=None) 
+
 libxl_sched_credit2_params = Struct("sched_credit2_params", [
     ("ratelimit_us", integer),
     ], dispose_fn=None)
 
 libxl_domain_remus_info = Struct("domain_remus_info",[
     ("interval",             integer),
+    ("timeout",              integer),
     ("allow_unsafe",         libxl_defbool),
     ("blackhole",            libxl_defbool),
     ("compression",          libxl_defbool),
@@ -984,6 +993,8 @@
     ("netbufscript",         string),
     ("diskbuf",              libxl_defbool),
     ("colo",                 libxl_defbool),
+    ("event_driven", 	     libxl_defbool),
+    ("polling",              libxl_defbool),
     ("userspace_colo_proxy", libxl_defbool)
     ])
 
diff -urBN xen/tools/xl/xl.h xen-4.10-rc5/tools/xl/xl.h
--- xen/tools/xl/xl.h	2017-12-15 14:52:52.757135000 +0100
+++ xen-4.10-rc5/tools/xl/xl.h	2017-11-21 17:46:07.083106000 +0100
@@ -148,6 +148,7 @@
 int main_sched_credit(int argc, char **argv);
 int main_sched_credit2(int argc, char **argv);
 int main_sched_rtds(int argc, char **argv);
+int main_sched_fp(int argc, char **argv);
 int main_domid(int argc, char **argv);
 int main_domname(int argc, char **argv);
 int main_rename(int argc, char **argv);
diff -urBN xen/tools/xl/xl_cmdtable.c xen-4.10-rc5/tools/xl/xl_cmdtable.c
--- xen/tools/xl/xl_cmdtable.c	2017-12-15 14:52:52.757135000 +0100
+++ xen-4.10-rc5/tools/xl/xl_cmdtable.c	2017-11-23 14:56:33.115106000 +0100
@@ -281,6 +281,18 @@
       "-b BUDGET, --budget=BUDGET     Budget (us)\n"
       "-e Extratime, --extratime=Extratime Extratime (1=yes, 0=no)\n"
     },
+    { "sched-fp",
+      &main_sched_fp, 0, 1,
+      "Get/set fp scheduler parameters",
+      "[-d <Domain> [-p[=PRIORITY]|-P[=PERIOD]|-s[=SLICE]]|-D[=DEADLINE]] [-S[=STRATEGY]]",
+      "-d DOMAIN, --domain=DOMAIN           Domain to modify\n"
+      "-p PRIORITY, --priority=PRIORITY     Priority of the specified domain (int)\n"
+      "-P PERIOD, --period=PERIOD           Period (int)\n"
+      "-s SLICE, --slice=SLICE              Slice (int)\n"
+      "-S STRATEGY, --strategy=STRATEGY     Strategy to be used by the scheduler (int)\n"
+      "                                      STRATEGY can either be 0 (rate-monotonic), 1 (deadline-monotonic) or 2 (fixed priority).\n"
+      "-D DEADLINE, --deadline=DEADLINE     Deadline (int)\n"
+    },
     { "domid",
       &main_domid, 0, 0,
       "Convert a domain name to domain id",
@@ -534,6 +546,8 @@
       "-c                      Enable COLO HA. It is conflict with -i and -b, and memory\n"
       "                        checkpoint must be disabled.\n"
       "-p                      Use COLO userspace proxy."
+      "-E                      Use event-driven instead of periodic checkpointing. Needs DomU support.\n"
+      "-t                      Timeout for heartbeat after which failover shall be triggered.\n"
     },
 #endif
     { "devd",
diff -urBN xen/tools/xl/xl_migrate.c xen-4.10-rc5/tools/xl/xl_migrate.c
--- xen/tools/xl/xl_migrate.c	2017-12-15 14:52:52.757135000 +0100
+++ xen-4.10-rc5/tools/xl/xl_migrate.c	2017-12-08 17:43:24.169135000 +0100
@@ -327,6 +327,11 @@
     char rc_buf;
     char *migration_domname;
     struct domain_create dom_info;
+    libxl_device_nic *nics;
+    struct timeval tv;
+    int nb, i;
+
+    FILE *log;
 
     signal(SIGPIPE, SIG_IGN);
     /* if we get SIGPIPE we'd rather just have it as an error */
@@ -400,6 +405,22 @@
                     "Failed to unpause domain %s (id: %u):%d\n",
                     ha, common_domname, domid, rc);
 
+        /*
+         * For a fast network failover we need to send a gratuitous
+         * arp request for all nics used by the restored domain.
+         */
+        nics = libxl_device_nic_list(ctx, domid, &nb);
+        if (nics && nb) {
+            for (i = 0; i < nb; ++i) {
+                libxl_device_nic_send_gratuitous_arp(ctx, &nics[i]);
+            }
+        }
+        log = fopen("/tmp/remus_trace.log", "a");
+        gettimeofday(&tv, NULL);
+        fprintf(log, "Domain resumed at backup. Time: %lu\n", tv.tv_sec*1000000+tv.tv_usec);
+
+        fclose(log);
+
         exit(rc ? EXIT_FAILURE : EXIT_SUCCESS);
     }
     default:
@@ -609,15 +630,21 @@
     libxl_domain_remus_info r_info;
     int send_fd = -1, recv_fd = -1;
     pid_t child = -1;
+    pid_t hb_child = -1;
+    pid_t hb_child_pgid;
     uint8_t *config_data;
     int config_len;
+    char *runhb;
 
     memset(&r_info, 0, sizeof(libxl_domain_remus_info));
 
-    SWITCH_FOREACH_OPT(opt, "Fbundi:s:N:ecp", NULL, "remus", 2) {
+    SWITCH_FOREACH_OPT(opt, "Fbundi:s:N:ecEpt:", NULL, "remus", 2) {
     case 'i':
         r_info.interval = atoi(optarg);
         break;
+    case 't':
+        r_info.timeout = atoi(optarg);
+        break;
     case 'F':
         libxl_defbool_set(&r_info.allow_unsafe, true);
         break;
@@ -645,6 +672,9 @@
     case 'c':
         libxl_defbool_set(&r_info.colo, true);
         break;
+    case 'E':
+        libxl_defbool_set(&r_info.event_driven, true);
+        break;
     case 'p':
         libxl_defbool_set(&r_info.userspace_colo_proxy, true);
     }
@@ -704,6 +734,7 @@
                           ssh_command, host,
                           "-r",
                           daemonize ? "" : " -e");
+                xasprintf(&runhb, "exec %s/heartbeat_launcher %s %i", LIBEXEC_BIN, host, r_info.timeout);
             } else {
                 xasprintf(&rune, "exec %s %s xl migrate-receive %s %s %s %s %s",
                           ssh_command, host,
@@ -718,6 +749,16 @@
 
         save_domain_core_begin(domid, NULL, &config_data, &config_len);
 
+        hb_child = fork();
+        
+        if (!hb_child) {
+            if (system(runhb) != 0) {
+                fprintf(stderr, "Could not initiate heartbeat. Aborting");
+                exit(1);
+            }
+            exit(0);
+        }
+
         if (!config_len) {
             fprintf(stderr, "No config file stored for running domain and "
                     "none supplied - cannot start remus.\n");
@@ -758,6 +799,17 @@
         libxl_domain_resume(ctx, domid, 1, 0);
     }
 
+    hb_child_pgid = getpgid(hb_child);
+    if (hb_child == -1) {
+        fprintf(stderr, "Something ist wrong here. Cannot kill child processes\n");
+        close(send_fd);
+        return EXIT_FAILURE;
+    }
+    if (kill(-hb_child_pgid, SIGTERM)) {
+        fprintf(stderr, "Unable to terminate heartbeat child processes. Killing them.\n");
+        kill(-hb_child_pgid, SIGKILL);
+    }
+
     close(send_fd);
     return EXIT_FAILURE;
 }
diff -urBN xen/tools/xl/xl_sched.c xen-4.10-rc5/tools/xl/xl_sched.c
--- xen/tools/xl/xl_sched.c	2017-12-15 14:52:52.757135000 +0100
+++ xen-4.10-rc5/tools/xl/xl_sched.c	2017-11-21 17:52:59.895106000 +0100
@@ -347,6 +347,85 @@
     return 0;
 }
 
+static int sched_fp_params_set(uint32_t poolid, libxl_sched_fp_params *scinfo)
+{
+    int rc;
+
+    rc = libxl_sched_fp_schedule_set(ctx, poolid, scinfo);
+    if (rc) 
+        fprintf(stderr, "libxl_sched_fp_params_set failed.\n");
+
+    return rc;
+}
+ 
+static int sched_fp_params_get(uint32_t poolid, libxl_sched_fp_params *scinfo)
+{
+    int rc;
+
+    rc = libxl_sched_fp_schedule_get(ctx, poolid, scinfo);
+    if (rc)
+        fprintf(stderr, "libxl_sched_fp_params_get failed.\n");
+    
+    return rc;
+}
+
+static int sched_fp_domain_output(int domid)
+{
+    char *domname;
+    libxl_domain_sched_params scinfo;
+    int rc;
+
+    if (domid < 0) {
+        printf("%-33s %4s %-4s %-4s %-4s %-4s\n", "Name", "ID", "Slice(us)", "Period(us)", "Deadline(us)", "Priority");
+        return 0;
+    }
+    libxl_domain_sched_params_init(&scinfo);
+    rc = sched_domain_get(LIBXL_SCHEDULER_FP, domid, &scinfo);
+    if (rc)
+        return rc;
+    domname = libxl_domid_to_name(ctx, domid);
+    printf("%-32s %5d %9u %10u %10u %8i\n",
+        domname,
+        domid,
+        (unsigned int)scinfo.slice,
+        (unsigned int)scinfo.period,
+        (unsigned int)scinfo.deadline,
+        (int)scinfo.priority);
+    free(domname);
+    libxl_domain_sched_params_dispose(&scinfo);
+    return 0;
+}
+    
+static int sched_fp_pool_output(uint32_t poolid)
+{
+    libxl_sched_fp_params scparam;
+    char *poolname;
+    char *strategy_name;
+    int rc;
+
+    poolname = libxl_cpupoolid_to_name(ctx, poolid);
+    rc = sched_fp_params_get(poolid, &scparam);
+    if (rc) {
+        printf("Cpupool: %s: [sched_params unavailable]\n", poolname);
+    } else {
+        switch (scparam.strategy) {
+            case LIBXL_SCHED_FP_STRAT_RM:
+                strategy_name = "rate-monotonic";
+                break;
+            case LIBXL_SCHED_FP_STRAT_DM:
+                strategy_name = "deadline-monotonic";
+                break; 
+            case LIBXL_SCHED_FP_STRAT_FP:
+                strategy_name = "fixed-priority";
+                break;
+        }
+        printf("Cpupool: %s: strategy=%s\n",poolname, strategy_name);
+    }
+    
+    free(poolname);
+    return 0;
+}
+    
 static int sched_domain_output(libxl_scheduler sched, int (*output)(int),
                                int (*pooloutput)(uint32_t), const char *cpupool)
 {
@@ -914,6 +993,153 @@
     return r;
 }
 
+/* Print a warning when hypothetical worst-case-load of a cpu may be higher
+ * than 1.0 (100%) and deadlines may be missed using the FP-Scheduler. */
+/*static void print_cpu_warnings(void)
+{
+    libxl_topologyinfo info;
+    libxl_sched_fp scinfo;
+
+    if (libxl_get_topologyinfo(&ctx, &info)) {
+        fprintf(stderr, "libxl_get_topologyinfo failed.\n");
+        return;
+    }
+
+    for (int i = 0; i < info.coremap.entries; i++) {
+        if (info.coremap.array[i] != LIBXL_CPUARRAY_INVALID_ENTRY) {
+            if ( !(libxl_sched_fp_get_wcload_on_cpu(&ctx, info.coremap.array[i], &scinfo))) {
+                if ( scinfo.load > 100 ) 
+                    printf("Warning on CPU %d: Load is higher than 1.0. Deadlines may be missed. \n Please consider rescheduling some domains manually.\n",i);
+            }
+        }
+    }
+}*/
+    
+
+int main_sched_fp(int argc, char **argv)
+{
+    const char *dom = NULL;
+    const char *cpupool = NULL;
+    int period = 0, slice = 0, deadline = 0, priority = 0, strategy = 0;
+    int opt_S = 0;
+    int opt_s = 0, opt_P = 0, opt_p = 0, opt_D = 0;
+    int opt, rc;
+    static struct option opts[] = {
+        {"domain", 1, 0, 'd'},
+        {"period", 1, 0, 'P'},
+        {"slice", 1, 0, 's'},
+        {"deadline", 1, 0, 'd'},
+        {"priority", 1, 0, 'p'},
+        {"strategy", 1, 0, 'S'},
+        {"cpupool", 1, 0, 'c'},
+        COMMON_LONG_OPTS,
+        {0,0,0,0}
+    };
+
+    SWITCH_FOREACH_OPT(opt, "d:P:s:D:p:S:c:h", opts, "sched-fp", 0) {
+    case 'd':
+        dom = optarg;
+        break;
+    case 'P':
+        period = strtol(optarg, NULL, 10);
+        opt_P = 1;
+        break;
+    case 's':
+        slice = strtol(optarg, NULL, 10);
+        opt_s = 1;
+        break;
+    case 'D':
+        deadline = strtol(optarg, NULL, 10);
+        opt_D = 1;
+        break;
+    case 'p':
+        priority = strtol(optarg, NULL, 10);
+        opt_p = 1;
+        break;
+    case 'S':
+        strategy = strtol(optarg, NULL, 10);
+        opt_S = 1;
+        break;
+    case 'c':
+        cpupool = optarg;
+        break;
+    }
+
+    if ((cpupool || opt_S) && (dom || opt_P || opt_s || opt_D || opt_p)) {
+        fprintf(stderr, "Cpupool or strategy may not be specified with domain options.\n");
+        return 1;
+    }
+
+    if (opt_S) {
+        libxl_sched_fp_params scparam;
+        uint32_t poolid = 0;
+
+        if (cpupool) {
+            if (libxl_cpupool_qualifier_to_cpupoolid(ctx, cpupool, &poolid, NULL) ||
+                !libxl_cpupoolid_is_valid(ctx, poolid)) {
+                fprintf(stderr, "unknown cpupool \'%s\'\n", cpupool);
+                return -ERROR_FAIL;
+            }
+        }
+
+        rc = sched_fp_params_get(poolid, &scparam);
+        if (rc)
+            return -rc;
+
+        scparam.strategy = strategy;
+
+        rc = sched_fp_params_set(poolid, &scparam);
+        if (rc)
+            return -rc;
+    } else if (!dom) {
+        return -sched_domain_output(LIBXL_SCHEDULER_FP, 
+                                    sched_fp_domain_output,
+                                    sched_fp_pool_output,
+                                    cpupool);
+    } else {
+        uint32_t domid = find_domain(dom);
+
+        if (!opt_P && !opt_p && !opt_s && !opt_D) {
+            sched_fp_domain_output(-1);
+            return -sched_fp_domain_output(domid);
+        } else {
+            libxl_domain_sched_params scinfo;
+            libxl_domain_sched_params_init(&scinfo);
+
+            scinfo.sched = LIBXL_SCHEDULER_FP;
+            rc = sched_domain_get(LIBXL_SCHEDULER_FP, domid, &scinfo);
+
+            if (rc) {
+                return -rc;
+            }
+
+            if (opt_P) 
+              scinfo.period = period;
+            if (opt_s)
+              scinfo.slice = slice;
+            if (opt_D)
+              scinfo.deadline = deadline;
+            if (opt_p) {
+                libxl_sched_fp_params scparam;
+                rc = sched_fp_params_get(0, &scparam);
+
+                if (scparam.strategy != LIBXL_SCHED_FP_STRAT_FP) {
+                    fprintf(stderr, "Specifying domain priority is only allowed with fixed-priority scheduling.\n");
+                    return 1; 
+                } else {
+                    scinfo.priority = priority;
+                }
+            }
+            rc = sched_domain_set(domid, &scinfo);
+            libxl_domain_sched_params_dispose(&scinfo);
+            if (rc)
+                return -rc;
+        }
+    }
+
+    return 0;
+}
+
 /*
  * Local variables:
  * mode: C
--- xen/xen/common/Makefile	2017-12-15 14:52:52.817135000 +0100
+++ xen-4.10-rc5/xen/common/Makefile	2017-11-21 17:16:33.207106000 +0100
@@ -41,6 +41,7 @@
 obj-$(CONFIG_SCHED_CREDIT2) += sched_credit2.o
 obj-$(CONFIG_SCHED_RTDS) += sched_rt.o
 obj-$(CONFIG_SCHED_NULL) += sched_null.o
+obj-y += sched_fp.o
 obj-y += schedule.o
 obj-y += shutdown.o
 obj-y += softirq.o
diff -urBN xen/xen/common/cpupool.c xen-4.10-rc5/xen/common/cpupool.c
--- xen/xen/common/cpupool.c	2017-12-15 14:52:52.817135000 +0100
+++ xen-4.10-rc5/xen/common/cpupool.c	2017-11-21 17:10:01.843106000 +0100
@@ -20,13 +20,11 @@
 #include <xen/keyhandler.h>
 #include <xen/cpu.h>
 
-#define for_each_cpupool(ptr)    \
-    for ((ptr) = &cpupool_list; *(ptr) != NULL; (ptr) = &((*(ptr))->next))
-
 struct cpupool *cpupool0;                /* Initial cpupool with Dom0 */
 cpumask_t cpupool_free_cpus;             /* cpus not in any cpupool */
 
-static struct cpupool *cpupool_list;     /* linked list, sorted by poolid */
+//static struct cpupool *cpupool_list;     /* linked list, sorted by poolid */
+struct cpupool *cpupool_list; 
 
 static int cpupool_moving_cpu = -1;
 static struct cpupool *cpupool_cpu_moving = NULL;
diff -urBN xen/xen/common/sched_fp.c xen-4.10-rc5/xen/common/sched_fp.c
--- xen/xen/common/sched_fp.c	1970-01-01 01:00:00.000000000 +0100
+++ xen-4.10-rc5/xen/common/sched_fp.c	2017-12-01 15:37:41.017698000 +0100
@@ -0,0 +1,1074 @@
+/* Preemptive RM/DM/Fixed-Priority Scheduler of Xen
+ *
+ * by Boguslaw Jablkowski, Michael Müller (C)  2014 Technische Universität Dortmund
+ * based on code of Credit and SEDF Scheduler
+ *
+ * This scheduler allows the usage of three scheduling strategies:
+ * - Rate-Monotonic
+ * - Deadline-Monotonic
+ * - and Fixed Priority.
+ * Furthermore it allows the dynamic switching of strategies on runtime.
+ */
+
+#include <xen/config.h>
+#include <xen/init.h>
+#include <xen/lib.h>
+#include <xen/sched.h>
+#include <xen/domain.h>
+#include <xen/delay.h>
+#include <xen/event.h>
+#include <xen/time.h>
+#include <xen/perfc.h>
+#include <xen/sched-if.h>
+#include <xen/softirq.h>
+#include <asm/atomic.h>
+#include <xen/errno.h>
+#include <xen/keyhandler.h>
+#include <xen/guest_access.h>
+
+/*Verbosity level
+ * 0 no information
+ * 1 print function calls
+ * 2 print additional infos
+ * 3 print ingoing and outgoing vcpu in do_schedule()
+ * 4 print run queue
+  */
+#define DLEVEL 2
+#define PRINT(_f, _a...)                        \
+    do {                                        \
+        if ( (_f) == DLEVEL )                \
+            printk(_a );                        \
+    } while ( 0 )
+
+
+/* Macros */
+#define FPSCHED_PRIV(_ops)   \
+    ((struct fpsched_private *)((_ops)->sched_data))
+#define CPU_INFO(cpu)  \
+    ((struct fp_cpu *)per_cpu(schedule_data, cpu).sched_priv)
+#define RUNQ(cpu)      (&(CPU_INFO(cpu)->runq))
+#define WAITQ(cpu)      (&(CPU_INFO(cpu)->waitq))
+#define LIST(_vcpu) (&_vcpu->queue_elem)
+#define FP_CPUONLINE(_pool)                                             \
+    (((_pool) == NULL) ? &cpupool_free_cpus : (_pool)->cpu_valid)
+#define FPSCHED_VCPU(_vcpu)  ((struct fp_vcpu *) (_vcpu)->sched_priv)
+#define FPSCHED_DOM(_dom)    ((struct fp_dom *) (_dom)->sched_priv)
+#define VM_SCHED_PRIO(_prio, _fpv) (FPSCHED_PRIV(ops)->strategy != FP? VM_STANDARD_PRIO((_fpv)) : (_prio))
+
+/* Default parameters for VM, dom0 and Idle Domain */
+/* Priorities */
+#define VM_DOM0_PRIO 1000
+#define VM_IDLE_PRIO 0
+
+/* Slices */
+#define VM_STANDARD_SLICE MICROSECS(500)
+#define VM_DOM0_SLICE  MICROSECS(900)
+
+/* Periods */
+#define VM_STANDARD_PERIOD MICROSECS(1000)
+#define VM_DOM0_PERIOD  MICROSECS(1000)
+
+/* Strategies */
+#define RM 0                    /* rate-monotonic */
+#define DM 1                    /* deadline-monotonic */
+#define FP 2                    /* fixed-priority */
+
+//static DEFINE_SPINLOCK(cpupool_lock);
+
+/* Print delay */
+#define DELTA 10000000000
+
+
+/*
+ * Physical CPU
+ */
+struct fp_cpu {
+    struct list_head runq;
+};
+
+/*
+ * Virtual CPU
+ */
+struct fp_vcpu {
+    struct list_head queue_elem;
+    struct vcpu *vcpu;
+    //struct fp_dom *sdom;
+    int priority;
+
+    s_time_t period;   /*=(relative deadline)*/
+    s_time_t slice;   /*=worst case execution time*/
+    s_time_t deadline;  /*=deadline*/
+
+    /*
+     * Bookkeeping
+     */
+    s_time_t period_next;
+    s_time_t last_time_scheduled;
+    s_time_t cputime;
+
+    unsigned long iterations;
+    s_time_t max_cputime;
+    s_time_t min_cputime;
+
+    long cputime_log[100];
+    int position;               /* position in run queue */
+};
+
+/*
+ * Domain
+ */
+struct fp_dom {
+    struct domain *domain;
+    int priority;
+    s_time_t period;
+    s_time_t slice;
+    s_time_t deadline;
+};
+
+/*
+ * Configuration structure.
+ * It consists of a compare function for vcpus and a priority handler.
+ * Each strategy has its own configuration structure instance.
+ */
+struct fp_strategy_conf {
+    int (*compare) (struct fp_vcpu *, struct fp_vcpu *);
+    void (*prio_handler) (struct domain *, int);
+};
+
+/* 
+ * System-wide scheduler data
+ */
+struct fpsched_private {
+    spinlock_t lock;
+    uint8_t strategy;           /* strategy to use */
+    struct fp_strategy_conf *config;
+    s_time_t last_time_temp;
+};
+
+/* DEBUG info */
+static inline void print_vcpu (struct fp_vcpu *fpv)
+{
+    PRINT (3, "c.d.v:%d.%d.%d, state: %d, p: %d, idle: %d, time: %lld \n",
+           fpv->vcpu->processor, fpv->vcpu->domain->domain_id,
+           fpv->vcpu->vcpu_id, fpv->vcpu->runstate.state, fpv->priority,
+           is_idle_vcpu (fpv->vcpu), (long long int)NOW ());
+}
+
+static inline void print_queue (struct list_head *const queue)
+{
+    struct fp_vcpu *snext;
+    struct list_head *iter;
+
+    list_for_each (iter, queue)
+    {
+        snext = list_entry (iter, struct fp_vcpu, queue_elem);
+
+        print_vcpu (snext);
+    }
+}
+
+/* List operations */
+static inline struct fp_vcpu *__runq_elem (struct list_head *elem)
+{
+    return list_entry (elem, struct fp_vcpu, queue_elem);
+}
+
+static inline int __vcpu_on_q (struct fp_vcpu *fpv)
+{
+    return !list_empty (&fpv->queue_elem);
+}
+
+static inline void __runq_remove (struct fp_vcpu *fpv)
+{
+    if (!is_idle_vcpu (fpv->vcpu))
+        list_del_init (&fpv->queue_elem);
+}
+
+static inline void
+__remove_from_queue (struct fp_vcpu *fpv, struct list_head *list)
+{
+    PRINT (1, "in remove_from_queue\n");
+    if (__vcpu_on_q (fpv))
+        list_del_init (&fpv->queue_elem);
+}
+
+/* 
+ * Insert a vcpu to the run queue of the given cpu using the function
+ * compare as sorting criteria.
+ */
+static inline void
+__runq_insert (unsigned int cpu, struct fp_vcpu *fpv,
+               int (*compare) (struct fp_vcpu *, struct fp_vcpu *))
+{
+    struct list_head *const runq = RUNQ (cpu);
+    struct list_head *iter;
+    int i = 0;
+
+//    PRINT (1, "in runq_insert: \n");
+//    if (cpu != fpv->vcpu->processor)
+//        return;
+    if (is_idle_vcpu (fpv->vcpu))
+        return;
+
+    PRINT (1, "CPU: %d, runq_insert, VPCU: %d \n", cpu, fpv->vcpu->vcpu_id);
+
+    list_for_each (iter, runq)
+    {
+        struct fp_vcpu *iter_fpv = __runq_elem (iter);
+
+        if (compare (fpv, iter_fpv))
+            break;
+    }
+    list_add_tail (&fpv->queue_elem, iter);
+
+    list_for_each (iter, runq)
+    {
+        struct fp_vcpu *iter_fpv = __runq_elem (iter);
+
+        iter_fpv->position = i;
+        i++;
+    }
+//    print_queue(runq);
+}
+
+/* Compare functions for the three scheduling strategies. */
+static int __runq_rm_compare (struct fp_vcpu *left, struct fp_vcpu *right)
+{
+    return left->period <= right->period;
+}
+
+static int __runq_dm_compare (struct fp_vcpu *left, struct fp_vcpu *right)
+{
+    return left->deadline <= right->deadline;
+}
+
+static int __runq_fp_compare (struct fp_vcpu *left, struct fp_vcpu *right)
+{
+    return left->priority >= right->priority;
+}
+
+/* 
+ * Calculating the priority of a domain and vcpu is performed as function
+ * of the used strategy. Each strategy has its own priority-handler that
+ * describes how the priority of a given domain and its vcpus will be 
+ * calculated.
+ */
+static void __fp_prio_handler (struct domain *dom, int priority)
+{
+    struct vcpu *v;
+    struct fp_dom *fp_dom = FPSCHED_DOM (dom);
+
+    PRINT (1, "in __fp_prio_handler\n");
+
+    fp_dom->priority = priority;
+
+    for_each_vcpu (dom, v)
+    {
+        struct fp_vcpu *fpv = FPSCHED_VCPU (v);
+
+        fpv->priority = priority;
+    }
+    PRINT (2, "Domain: %d Period: %d Deadline %d Priority: %d\n",
+           dom->domain_id, (int)fp_dom->period, (int)fp_dom->deadline,
+           fp_dom->priority);
+}
+
+static void __rm_prio_handler (struct domain *dom, int priority)
+{
+    int posacc = 0;
+    int count = 0;
+    struct vcpu *v;
+    struct fp_dom *fp_dom = FPSCHED_DOM (dom);
+
+    PRINT (1, "in __rm_prio_handler\n");
+
+    if (dom->domain_id == 0)
+    {
+        priority = VM_DOM0_PRIO;
+    }
+    else if (is_idle_domain (dom))
+    {
+        priority = VM_IDLE_PRIO;
+    }
+    else
+    {
+
+        for_each_vcpu (dom, v)
+        {
+            struct fp_vcpu *fpv = FPSCHED_VCPU (v);
+
+            posacc += fpv->position;
+            count++;
+        }
+        priority = VM_DOM0_PRIO - posacc / count - 1;
+    }
+
+    fp_dom->priority = priority;
+
+    for_each_vcpu (dom, v)
+    {
+        struct fp_vcpu *fpv = FPSCHED_VCPU (v);
+
+        fpv->priority = priority;
+    }
+    PRINT (2, "Domain: %d Period: %d Deadline %d Priority: %d\n",
+           dom->domain_id, (int)fp_dom->period, (int)fp_dom->deadline,
+           fp_dom->priority);
+}
+
+static void __dm_prio_handler (struct domain *dom, int priority)
+{
+    int posacc = 0;
+    int count = 0;
+    struct vcpu *v;
+    struct fp_dom *fp_dom = FPSCHED_DOM (dom);
+
+    PRINT (1, "in __dm_prio_handler\n");
+
+    if (dom->domain_id == 0)
+    {
+        priority = VM_DOM0_PRIO;
+    }
+    else if (is_idle_domain (dom))
+    {
+        priority = VM_IDLE_PRIO;
+    }
+    else
+    {
+
+        for_each_vcpu (dom, v)
+        {
+            struct fp_vcpu *fpv = FPSCHED_VCPU (v);
+
+            posacc += fpv->position;
+            count++;
+        }
+        priority = VM_DOM0_PRIO - posacc / count - 1;
+    }
+
+    fp_dom->priority = priority;
+
+    for_each_vcpu (dom, v)
+    {
+        struct fp_vcpu *fpv = FPSCHED_VCPU (v);
+
+        fpv->priority = priority;
+    }
+    PRINT (2, "Domain: %d Period: %d Deadline %d Priority: %d\n",
+           dom->domain_id, (int)fp_dom->period, (int)fp_dom->deadline,
+           fp_dom->priority);
+}
+
+/* 
+ * Calculate the hypothetical worst-case load of the given cpu.
+ * This is used as backend to provide the user with a warning when
+ * deadlines may be missed due to overloading a cpu.
+ * NOTE: Currently broken.
+ * TODO: Fix in later release.
+ */
+/*static uint32_t fp_get_wcload_on_cpu (int cpu)
+{
+    struct list_head *const runq = RUNQ (cpu);
+    struct list_head *iter;
+    uint32_t load = 0;
+
+    list_for_each (iter, runq)
+    {
+        const struct fp_vcpu *const iter_fpv = __runq_elem (iter);
+
+        load += iter_fpv->slice * 100 / iter_fpv->period;
+    }
+    return load;
+}*/
+
+/* Reinsert a vcpu to a run queue. */
+static void
+fp_reinsertsort_vcpu (struct vcpu *vc,
+                      int (*compare) (struct fp_vcpu *, struct fp_vcpu *))
+{
+    const int cpu = vc->processor;
+    struct fp_vcpu *fpv = vc->sched_priv;
+    struct list_head *const runq = RUNQ (cpu);
+
+    __remove_from_queue (fpv, runq);
+    __runq_insert (cpu, fpv, compare);
+    cpu_raise_softirq (cpu, SCHEDULE_SOFTIRQ);
+}
+
+/* Get the currently used strategy. */
+static int
+fp_sched_get (const struct scheduler *ops,
+              struct xen_sysctl_fp_schedule *schedule)
+{
+    struct fpsched_private *prv = FPSCHED_PRIV (ops);
+
+    schedule->strategy = prv->strategy;
+
+    return 0;
+}
+
+/* 
+ * Set the new strategy and reschedule all domains and
+ * recalculate their priorities accordingly. Also set
+ * the new compare function and priority handler.
+ */
+static int
+fp_sched_set (const struct scheduler *ops,
+              struct xen_sysctl_fp_schedule *schedule)
+{
+    struct fpsched_private *prv = FPSCHED_PRIV (ops);
+
+    /*
+     * Check if given strategy is valid. schedule->strategy is valid if it is either
+     * 0 for rate-monotonic, 1 for deadline-monotonic or 2 for fixed_priority 
+     */
+    if (schedule->strategy < 0 || schedule->strategy > 2)
+        return -EINVAL;
+
+    /*
+     * Addopt the new strategy 
+     */
+    prv->strategy = schedule->strategy;
+    switch (prv->strategy)
+    {
+    case 0:
+    {
+        prv->config->compare = __runq_rm_compare;
+        prv->config->prio_handler = __rm_prio_handler;
+        break;
+    }
+    case 1:
+    {
+        prv->config->compare = __runq_dm_compare;
+        prv->config->prio_handler = __dm_prio_handler;
+        break;
+    }
+    case 2:
+    {
+        prv->config->compare = __runq_fp_compare;
+        prv->config->prio_handler = __fp_prio_handler;
+    }
+    }
+    PRINT (2, "Strategy is now %d\n", prv->strategy);
+
+    return 0;
+}
+
+/* Recalculate priorities after updating scheduler or domain parameters. */ 
+static void
+fp_sched_set_vm_prio (const struct scheduler *ops, struct domain *d, int prio)
+{
+    struct fp_dom *fpd = FPSCHED_DOM (d);
+    int strategy = FPSCHED_PRIV (ops)->strategy;
+    struct vcpu *v;
+
+
+    PRINT (1, "in fp_sched_set_vm_prio\n");
+    PRINT (2, "in fp_sched_set_vm_prio, domain %d, strategy %d\n", d->domain_id, strategy);
+
+    if (d->domain_id == 0)
+    {
+        fpd->priority = VM_DOM0_PRIO;
+        return;
+    }
+    else if (is_idle_domain (d))
+    {
+        fpd->priority = VM_IDLE_PRIO;
+        return;
+    }
+    PRINT (2, "in fp_sched_set_vm_prio, fpd->priority %d\n", fpd->priority);
+
+    for_each_vcpu (d, v)
+    {
+//        struct fp_vcpu *fpv = FPSCHED_VCPU (v);
+
+        if (d->domain_id == 0 || is_idle_domain (d))
+        {
+//            fpv->priority = fpd->priority;
+                continue;
+        }
+/*        if (strategy != FP)
+        {
+            FPSCHED_PRIV (ops)->config->prio_handler (d, prio);
+            fp_reinsertsort_vcpu (v, FPSCHED_PRIV (ops)->config->compare);
+        }*/
+        FPSCHED_PRIV (ops)->config->prio_handler (d, prio);
+        fp_reinsertsort_vcpu (v, FPSCHED_PRIV (ops)->config->compare);
+    }
+}
+
+static void fp_insert_vcpu (const struct scheduler *ops, struct vcpu *vc)
+{
+    struct fp_vcpu *fpv = vc->sched_priv;
+    spinlock_t *lock;
+
+    BUG_ON( is_idle_vcpu(vc) );
+
+    PRINT (1, "in fp_insert_vcpu\n");
+    PRINT (2, "in fp_insert_vcpu %d\n", vc->vcpu_id);
+
+    lock = vcpu_schedule_lock_irq(vc);
+
+    if (!__vcpu_on_q (fpv) && vcpu_runnable (vc) && !vc->is_running)
+    {
+        __runq_insert (vc->processor, fpv, FPSCHED_PRIV (ops)->config->compare);
+        if (FPSCHED_PRIV (ops)->strategy != FP)
+            FPSCHED_PRIV (ops)->config->prio_handler (vc->domain, 1);
+    }
+    vcpu_schedule_unlock_irq(lock, vc);
+}
+
+static void *fp_alloc_domdata (const struct scheduler *ops, struct domain *d)
+{
+    struct fp_dom *fp_dom;
+
+    PRINT (1, "in alloc domdata\n");
+    PRINT (2, "in alloc domdata, domainID: %d\n", d->domain_id);
+
+    fp_dom = xmalloc (struct fp_dom);
+
+    if (fp_dom == NULL)
+        return NULL;
+    memset (fp_dom, 0, sizeof (*fp_dom));
+
+    return (void *)fp_dom;
+}
+
+static int fp_init_domain (const struct scheduler *ops, struct domain *d)
+{
+    struct fp_dom *fp_dom;
+
+    PRINT (1, "in init_domain\n");
+    PRINT (2, "in init_domain %d\n", d->domain_id);
+    if (is_idle_domain (d))
+        return 0;
+
+    fp_dom = fp_alloc_domdata (ops, d);
+    if (d == NULL)
+        return -ENOMEM;
+
+    d->sched_priv = fp_dom;
+
+    fp_dom->domain = d;
+
+    if (d->domain_id == 0)
+    {
+        fp_dom->priority = VM_DOM0_PRIO;
+        fp_dom->slice = VM_DOM0_SLICE;
+        fp_dom->period = VM_DOM0_PERIOD;
+        fp_dom->deadline = VM_DOM0_PERIOD;      /* assume rate-monotonic scheduling for default */
+    }
+    else
+    {
+        fp_dom->period = VM_STANDARD_PERIOD;
+        fp_dom->deadline = VM_STANDARD_PERIOD;
+        fp_dom->slice = is_idle_domain (d) ? MICROSECS (0) : VM_STANDARD_SLICE;
+
+        if (is_idle_domain (d))
+        {
+            fp_dom->priority = VM_IDLE_PRIO;
+        }
+    }
+    return 0;
+}
+
+static int fp_init (struct scheduler *ops)
+{
+    struct fpsched_private *prv;
+    struct fp_strategy_conf *conf;
+
+    printk("Initializing FP scheduler\n"
+           "WARNING: This is experimental software in development.\n"
+           "Use at your own risk.\n");
+
+    prv = xmalloc (struct fpsched_private);
+
+    if (prv == NULL)
+        return -ENOMEM;
+    conf = xmalloc (struct fp_strategy_conf);
+    if (conf == NULL)
+        return -ENOMEM;
+
+    memset (prv, 0, sizeof (*prv));
+    memset (prv, 0, sizeof (*conf));
+    ops->sched_data = prv;
+    spin_lock_init(&prv->lock);
+
+    prv->strategy = 0;
+    prv->config = conf;
+    prv->config->compare = __runq_rm_compare;
+    prv->config->prio_handler = __rm_prio_handler;
+    prv->last_time_temp =0;
+
+    return 0;
+}
+
+static void fp_deinit (struct scheduler *ops)
+{
+    PRINT (1, "in fp_deinit\n");
+    xfree (FPSCHED_PRIV (ops)->config);
+    xfree (FPSCHED_PRIV (ops));
+    ops->sched_data = NULL;
+}
+
+static void fp_init_pdata(const struct scheduler *ops, void *pdata, int cpu)
+{
+    struct fpsched_private *prv = FPSCHED_PRIV(ops);
+    spinlock_t *old_lock;
+    unsigned long flags;
+
+    old_lock = pcpu_schedule_lock_irqsave(cpu, &flags);
+    
+    per_cpu(schedule_data, cpu).schedule_lock = &prv->lock;
+    
+    spin_unlock_irqrestore(old_lock, flags);
+} 
+
+static void fp_free_domdata (const struct scheduler *ops, void *data)
+{
+    PRINT (1, "in fp_free_domdata\n");
+    xfree (data);
+}
+
+static void fp_destroy_domain (const struct scheduler *ops, struct domain *d)
+{
+    PRINT (1, "in fp_destroy_domain\n");
+    fp_free_domdata (ops, d->sched_priv);
+}
+
+static void fp_free_vdata (const struct scheduler *ops, void *priv)
+{
+    struct fp_vcpu *fpv = priv;
+
+    PRINT (1, "in fp_free_vdata\n");
+    xfree (fpv);
+}
+
+static void *fp_alloc_vdata (const struct scheduler *ops, struct vcpu *vc,
+                             void *dd)
+{
+    struct fp_vcpu *fpv;
+    struct fp_dom *fp_dom = FPSCHED_DOM (vc->domain);
+
+    PRINT (1, "in alloc_vdata\n");
+    PRINT (2, "in alloc_vdata, vc->domainID: %d\n", vc->domain->domain_id);
+
+    fpv = xmalloc (struct fp_vcpu);
+
+    if (fpv == NULL)
+        return NULL;
+    memset (fpv, 0, sizeof (*fpv));
+
+    fpv->vcpu = vc;
+
+    if (fp_dom != NULL)
+    {
+        fpv->slice = fp_dom->slice;
+        fpv->period = fp_dom->period;
+        fpv->priority = fp_dom->priority;
+        fpv->deadline = fp_dom->deadline;
+    }
+    else
+    {
+        if (vc->domain->domain_id == 0)
+        {
+            fpv->priority = VM_DOM0_PRIO;
+            fpv->slice = VM_DOM0_SLICE;
+            fpv->period = VM_DOM0_PERIOD;
+            fpv->deadline = fpv->period;
+        }
+        else
+        {
+            fpv->period = VM_STANDARD_PERIOD;
+            fpv->deadline = fpv->period;
+            if (is_idle_domain (vc->domain))
+            {
+                fpv->priority = VM_IDLE_PRIO;
+            }
+            fpv->slice =
+                is_idle_domain (vc->domain) ? MICROSECS (0) : VM_STANDARD_SLICE;
+        }
+    }
+
+    fpv->cputime = 0;
+    fpv->last_time_scheduled = 0;
+    fpv->period_next = NOW () + fpv->period;
+    fpv->iterations = 0;
+
+    INIT_LIST_HEAD (&fpv->queue_elem);
+    return fpv;
+}
+
+static int fp_pick_cpu (const struct scheduler *ops, struct vcpu *v)
+{
+    cpumask_t online_affinity;
+    cpumask_t *online;
+    int cpu;
+
+    PRINT (1, "in rt_pick_cpu, v->cpu: %d, pick up cpu: ", v->processor);
+
+    online = cpupool_domain_cpumask(v->domain);
+    cpumask_and(&online_affinity, online, v->cpu_hard_affinity);
+
+    cpu = cpumask_test_cpu(v->processor, &online_affinity)
+            ? v->processor
+            : cpumask_cycle(v->processor, &online_affinity);
+    PRINT (1, "%d\n", cpu);
+    return cpu;
+}
+
+static int
+fp_adjust_global (const struct scheduler *ops,
+                  struct xen_sysctl_scheduler_op *sc)
+{
+    xen_sysctl_fp_schedule_t local_sched;
+    int rc = -EINVAL;
+    struct domain *d;
+    struct cpupool *c = xzalloc(struct cpupool);
+    struct cpupool **q;
+
+    PRINT (1, "in fp_adjust_global\n");
+
+    switch (sc->cmd)
+    {
+    case XEN_SYSCTL_SCHEDOP_putinfo:
+        copy_from_guest (&local_sched, sc->u.sched_fp.schedule, 1);
+        rc = fp_sched_set (ops, &local_sched);
+        break;
+    case XEN_SYSCTL_SCHEDOP_getinfo:
+        rc = fp_sched_get (ops, &local_sched);
+        copy_to_guest (sc->u.sched_fp.schedule, &local_sched, 1);
+        break;
+    }
+
+    for_each_cpupool(q)
+    {
+        if((*q)->sched->sched_id == XEN_SCHEDULER_FP)
+        {
+            c = *q;
+
+            for_each_domain_in_cpupool(d, c)
+            {
+                struct fp_dom *const fpd = FPSCHED_DOM (d);
+                fp_sched_set_vm_prio (ops, d, fpd->priority);
+            }
+        }
+    }
+    return rc;
+}
+
+static void fp_free_pdata (const struct scheduler *ops, void *spc, int cpu)
+{
+    PRINT (1, "in fp_free_pdata\n");
+    if (spc == NULL)
+        return;
+    xfree (spc);
+}
+
+static void *fp_alloc_pdata (const struct scheduler *ops, int cpu)
+{
+    struct fp_cpu *fpc;
+
+    PRINT (1, "in alloc_pdata\n");
+    PRINT (2, "CPU %d in alloc_pdata\n", cpu);
+
+    fpc = xmalloc (struct fp_cpu);
+
+    memset (fpc, 0, sizeof (*fpc));
+
+    INIT_LIST_HEAD (&fpc->runq);
+    return fpc;
+}
+
+static void fp_sleep (const struct scheduler *ops, struct vcpu *vc)
+{
+    struct fp_vcpu *const fpv = FPSCHED_VCPU (vc);
+    const unsigned int cpu = vc->processor;
+
+    PRINT (1, "in fp_sleep\n");
+    PRINT (2, "CPU %d in fp_sleep\n", cpu);
+
+    if (is_idle_vcpu (vc))
+        return;
+
+    if (per_cpu (schedule_data, cpu).curr == vc)
+    {
+        cpu_raise_softirq (cpu, SCHEDULE_SOFTIRQ);
+    }
+    else if (__vcpu_on_q (fpv))
+        __runq_remove (fpv);
+}
+
+static void fp_vcpu_wake (const struct scheduler *ops, struct vcpu *vc)
+{
+    struct fp_vcpu *const fpv = FPSCHED_VCPU (vc);
+    const unsigned int cpu = vc->processor;
+    struct fpsched_private *prv = FPSCHED_PRIV (ops);
+    s_time_t now = NOW();
+
+    PRINT (3, "in fp_vcpu_wake, CPU: %d, \n", cpu);
+
+    if (unlikely (per_cpu (schedule_data, vc->processor).curr == vc))
+        return;
+    if (unlikely (is_idle_vcpu (vc)))
+        return;
+    if (unlikely (__vcpu_on_q (fpv)))
+    {
+        if( now < prv->last_time_temp + DELTA && vc->processor == 7) 
+        {
+            PRINT(1, "in fp_vcpu_wake, CPU: %d, \n", cpu);
+            prv->last_time_temp = now;
+        }
+        return;
+    }
+
+    __runq_insert (cpu, fpv, FPSCHED_PRIV (ops)->config->compare);
+    FPSCHED_PRIV (ops)->config->prio_handler (vc->domain, fpv->priority);
+    cpu_raise_softirq (cpu, SCHEDULE_SOFTIRQ);
+}
+
+static void fp_vcpu_remove (const struct scheduler *ops, struct vcpu *vc)
+{
+    struct fp_vcpu *const fpv = FPSCHED_VCPU (vc);
+    const unsigned int cpu = vc->processor;
+    struct list_head *const runq = RUNQ (cpu);
+
+    PRINT (1, "in fp_vcpu_remove\n");
+    PRINT (2, "CPU: %d in fp_vcpu_remove\n", cpu);
+    __remove_from_queue (fpv, runq);
+}
+
+static int
+fp_adjust (const struct scheduler *ops, struct domain *d,
+           struct xen_domctl_scheduler_op *op)
+{
+    struct fp_dom *const fp_dom = FPSCHED_DOM (d);
+    struct vcpu *v;
+    struct domain *dom;
+    struct cpupool *c = xzalloc(struct cpupool);
+    struct cpupool **q;
+
+
+    PRINT (1, "in fp_adjust\n");
+    PRINT (2, "in fp_adjust, cpupool id: %d, cpupool->n_dom %d\n", d->cpupool->cpupool_id, d->cpupool->n_dom);
+    if (op->cmd == XEN_DOMCTL_SCHEDOP_getinfo)
+    {
+        op->u.fp.priority = fp_dom->priority;
+        op->u.fp.slice = fp_dom->slice;
+        op->u.fp.period = fp_dom->period;
+        op->u.fp.deadline = fp_dom->deadline;
+    }
+    else
+    {
+        if (op->u.fp.period > 0)
+        {
+            fp_dom->period = op->u.fp.period * 1000;
+
+            if (FPSCHED_PRIV (ops)->strategy == RM)
+                fp_dom->deadline = fp_dom->period;
+            for_each_vcpu (d, v)
+            {
+                struct fp_vcpu *fpv = FPSCHED_VCPU (v);
+
+                fpv->period = op->u.fp.period * 1000;
+                if (FPSCHED_PRIV (ops)->strategy == RM)
+                    fpv->deadline = fpv->period;
+            }
+        }
+        if (op->u.fp.slice > 0)
+        {
+            fp_dom->slice = op->u.fp.slice * 1000;
+            for_each_vcpu (d, v)
+            {
+                FPSCHED_VCPU (v)->slice = op->u.fp.slice * 1000;
+            }
+        }
+        if (op->u.fp.deadline > 0)
+        {
+            fp_dom->deadline = op->u.fp.deadline * 1000;
+            for_each_vcpu (d, v)
+            {
+                FPSCHED_VCPU (v)->deadline = op->u.fp.deadline * 1000;
+            }
+        }
+        /*
+         * Schedule must be updated always now. Otherwise changing period or deadline does not cause a change in priority when using
+         * rate-monotonic or deadline-monotonic scheduling. 
+         */
+
+         if (d->cpupool->sched->sched_id == XEN_SCHEDULER_FP) 
+            fp_sched_set_vm_prio (ops, d,
+                                  op->u.fp.priority >
+                                  0 ? op->u.fp.priority : fp_dom->priority);
+
+        for_each_cpupool(q)
+        {
+            if((*q)->sched->sched_id == XEN_SCHEDULER_FP)
+            {
+                c = *q;
+
+                for_each_domain_in_cpupool(dom, c)
+                {
+                     struct fp_dom *const fpd = FPSCHED_DOM (dom);
+                     if(d->domain_id == dom->domain_id) continue;
+                     fp_sched_set_vm_prio (ops, dom, fpd->priority);
+                }
+            }
+        }
+
+    }
+    return 0;
+}
+
+
+static void update_queue (s_time_t now, struct list_head *runq)
+{
+    struct list_head *iter;
+
+    if (!list_empty (runq))
+    {
+        list_for_each (iter, runq)
+        {
+            struct fp_vcpu *fpv = __runq_elem (iter);
+
+            if (now > fpv->period_next)
+            {
+                fpv->iterations = fpv->iterations + 1;
+                /*
+                 * printk("core.dom.vcpu:%d.%d.%d, cputime: %ld, max_ct: %ld, last_schedule: %ld, period_next: %ld, time: %ld, period: %ld, slice: %ld\n",
+                 * fpv->vcpu->processor,fpv->vcpu->domain->domain_id, fpv->vcpu->vcpu_id,
+                 * fpv->cputime, fpv->max_cputime, fpv->last_time_scheduled, fpv->period_next, now, fpv->period, fpv->slice);
+                 */
+                fpv->cputime = 0;
+                fpv->period_next = now + fpv->period;
+            }
+        }
+    }
+
+}
+
+
+static struct task_slice
+fp_do_schedule (const struct scheduler *ops, s_time_t now,
+                bool_t tasklet_work_scheduled)
+{
+    const int cpu = smp_processor_id ();
+    struct list_head *const runq = RUNQ (cpu);
+    struct fp_vcpu *cur = FPSCHED_VCPU (current);
+    struct fp_vcpu *snext = NULL;
+    struct task_slice ret = {.migrated = 0};
+    struct list_head *iter;
+
+    if (!is_idle_vcpu (current))
+    {
+        cur->cputime += now - cur->last_time_scheduled;
+    }
+    update_queue (now, runq);
+
+    /* Get next runnable vcpu */
+    if (!list_empty (runq))
+    {
+        list_for_each (iter, runq)
+        {
+            const struct fp_vcpu *const iter_fpv = __runq_elem (iter);
+
+            if (vcpu_runnable (iter_fpv->vcpu) 
+                && (iter_fpv->cputime < iter_fpv->slice))
+
+            {
+                snext = __runq_elem (iter);
+                snext->last_time_scheduled = now;
+                break;
+            }
+        }
+    }
+    if (snext == NULL)
+        snext = FPSCHED_VCPU (idle_vcpu[cpu]);
+
+    if (tasklet_work_scheduled)
+    {
+        PRINT (1, "Tasklet work:\n");
+        snext = FPSCHED_VCPU (idle_vcpu[cpu]);
+    }
+
+//    ret.time = MICROSECS (50);   //MILLISECS(1);
+//    ret.time = MICROSECS (snext->slice - snext->cputime); //MILLISECS(1); //MICROSECS (10);   //MILLISECS(1);
+    ret.time = MICROSECS (100);   //MILLISECS(1);
+    ret.task = snext->vcpu;
+    return ret;
+}
+
+
+/* Change the scheduler of cpu to us (FP). */
+static void
+fp_switch_sched(struct scheduler *new_ops, unsigned int cpu,
+                void *pdata, void *vdata)
+{
+    struct schedule_data *sd = &per_cpu(schedule_data, cpu);
+    struct fp_vcpu *vc = vdata;
+
+    PRINT (1, "in fp_switch_sched\n");
+    PRINT (2, "in fp_switch_sched: cpu %d\n", cpu);
+
+    ASSERT(vc && is_idle_vcpu(vc->vcpu));
+
+    ASSERT(sd->schedule_lock == &sd->_lock && !spin_is_locked(&sd->_lock));
+
+    idle_vcpu[cpu]->sched_priv = vdata;
+
+    per_cpu(scheduler, cpu) = new_ops;
+    per_cpu(schedule_data, cpu).sched_priv = pdata ;//NULL; /* no pdata */
+
+    smp_mb();
+    sd->schedule_lock = &sd->_lock;
+
+
+}
+
+//struct fpsched_private _fpsched_private;
+
+static const struct scheduler sched_fp_def = {
+    .name = "Fixed Priority Scheduler",
+    .opt_name = "fp",
+    .sched_id = XEN_SCHEDULER_FP,
+    .sched_data = NULL, //&_fpsched_private,
+
+    .init_domain = fp_init_domain,
+    .destroy_domain = fp_destroy_domain,
+
+    .insert_vcpu = fp_insert_vcpu,
+    .remove_vcpu = fp_vcpu_remove,
+
+    .sleep = fp_sleep,
+    .yield = NULL,
+    .wake = fp_vcpu_wake,
+    
+    .adjust = fp_adjust,
+    .adjust_global = fp_adjust_global,
+    
+    .pick_cpu = fp_pick_cpu,
+    .do_schedule = fp_do_schedule,
+    
+    .dump_cpu_state = NULL,
+    .dump_settings = NULL,
+    .init = fp_init,
+    .deinit = fp_deinit,
+    .alloc_vdata = fp_alloc_vdata,
+    .free_vdata = fp_free_vdata,
+
+    .alloc_pdata = fp_alloc_pdata,
+    .free_pdata = fp_free_pdata,
+
+    .init_pdata     = fp_init_pdata,
+    .switch_sched   = fp_switch_sched,
+//    .deinit_pdata   = fp_deinit_pdata,
+
+
+    .alloc_domdata = fp_alloc_domdata,
+    .free_domdata = fp_free_domdata,
+
+    .tick_suspend = NULL,
+    .tick_resume = NULL,
+};
+
+REGISTER_SCHEDULER(sched_fp_def);
diff -urBN xen/xen/common/schedule.c xen-4.10-rc5/xen/common/schedule.c
--- xen/xen/common/schedule.c	2017-12-15 14:52:52.829135000 +0100
+++ xen-4.10-rc5/xen/common/schedule.c	2017-12-01 17:25:36.068873000 +0100
@@ -180,6 +180,9 @@
     s_time_t delta;
 
     ASSERT(v->runstate.state != new_state);
+	if (!(spin_is_locked(per_cpu(schedule_data,v->processor).schedule_lock))) {
+		printk("schedule.c:vcpu_runstate_change Lock at %p not set when chaging state of vcpu to %d\n",(void*)(per_cpu(schedule_data,v->processor).schedule_lock), new_state);
+	}
     ASSERT(spin_is_locked(per_cpu(schedule_data,v->processor).schedule_lock));
 
     vcpu_urgent_count_update(v);
@@ -1390,6 +1393,7 @@
     spinlock_t           *lock;
     struct task_slice     next_slice;
     int cpu = smp_processor_id();
+	int new_state;
 
     ASSERT_NOT_IN_ATOMIC();
 
@@ -1457,14 +1461,17 @@
     TRACE_4D(TRC_SCHED_SWITCH,
              prev->domain->domain_id, prev->vcpu_id,
              next->domain->domain_id, next->vcpu_id);
+	new_state = ((prev->pause_flags & VPF_blocked) ? RUNSTATE_blocked :
+         (vcpu_runnable(prev) ? RUNSTATE_runnable : RUNSTATE_offline));
 
     vcpu_runstate_change(
-        prev,
-        ((prev->pause_flags & VPF_blocked) ? RUNSTATE_blocked :
-         (vcpu_runnable(prev) ? RUNSTATE_runnable : RUNSTATE_offline)),
+        prev, new_state,
         now);
     prev->last_run_time = now;
 
+	if (next->runstate.state == RUNSTATE_running) {
+		printk("Domain already in state running when being scheduled: %d on cpu %d\n", next->domain->domain_id, cpu);
+	}
     ASSERT(next->runstate.state != RUNSTATE_running);
     vcpu_runstate_change(next, RUNSTATE_running, now);
 
diff -urBN xen/xen/common/timer.c xen-4.10-rc5/xen/common/timer.c
--- xen/xen/common/timer.c	2017-12-15 14:52:52.829135000 +0100
+++ xen-4.10-rc5/xen/common/timer.c	2017-11-21 17:10:01.847106000 +0100
@@ -25,7 +25,7 @@
 #include <asm/atomic.h>
 
 /* We program the time hardware this far behind the closest deadline. */
-static unsigned int timer_slop __read_mostly = 50000; /* 50 us */
+static unsigned int timer_slop __read_mostly = 10000; /* 10 us */
 integer_param("timer_slop", timer_slop);
 
 struct timers {
diff -urBN xen/xen/include/public/domctl.h xen-4.10-rc5/xen/include/public/domctl.h
--- xen/xen/include/public/domctl.h	2017-12-15 14:52:52.877135000 +0100
+++ xen-4.10-rc5/xen/include/public/domctl.h	2017-11-21 17:24:22.791106000 +0100
@@ -328,6 +328,7 @@
 #define XEN_SCHEDULER_ARINC653 7
 #define XEN_SCHEDULER_RTDS     8
 #define XEN_SCHEDULER_NULL     9
+#define XEN_SCHEDULER_FP      10
 
 struct xen_domctl_sched_credit {
     uint16_t weight;
@@ -348,11 +349,19 @@
     uint32_t flags;
 };
 
+typedef struct xen_domctl_sched_fp {
+    uint64_aligned_t slice;
+    uint64_aligned_t period;
+    uint64_aligned_t deadline;
+    int32_t priority;
+} xen_domctl_sched_fp_t;
+
 typedef struct xen_domctl_schedparam_vcpu {
     union {
         struct xen_domctl_sched_credit credit;
         struct xen_domctl_sched_credit2 credit2;
         struct xen_domctl_sched_rtds rtds;
+	struct xen_domctl_sched_fp fp;
     } u;
     uint32_t vcpuid;
 } xen_domctl_schedparam_vcpu_t;
@@ -382,6 +391,7 @@
         struct xen_domctl_sched_credit credit;
         struct xen_domctl_sched_credit2 credit2;
         struct xen_domctl_sched_rtds rtds;
+	struct xen_domctl_sched_fp fp;
         struct {
             XEN_GUEST_HANDLE_64(xen_domctl_schedparam_vcpu_t) vcpus;
             /*
diff -urBN xen/xen/include/public/sysctl.h xen-4.10-rc5/xen/include/public/sysctl.h
--- xen/xen/include/public/sysctl.h	2017-12-15 14:52:52.881135000 +0100
+++ xen-4.10-rc5/xen/include/public/sysctl.h	2017-11-21 17:26:57.995106000 +0100
@@ -595,6 +595,15 @@
 #define XEN_SYSCTL_SCHED_RATELIMIT_MAX 500000
 #define XEN_SYSCTL_SCHED_RATELIMIT_MIN 100
 
+
+struct xen_sysctl_fp_schedule {
+    uint8_t strategy;
+    uint32_t load;
+};
+
+typedef struct xen_sysctl_fp_schedule xen_sysctl_fp_schedule_t;
+DEFINE_XEN_GUEST_HANDLE(xen_sysctl_fp_schedule_t);
+
 struct xen_sysctl_credit_schedule {
     /* Length of timeslice in milliseconds */
 #define XEN_SYSCTL_CSCHED_TSLICE_MAX 1000
@@ -614,6 +623,7 @@
 struct xen_sysctl_scheduler_op {
     uint32_t cpupool_id; /* Cpupool whose scheduler is to be targetted. */
     uint32_t sched_id;   /* XEN_SCHEDULER_* (domctl.h) */
+    uint32_t cpu;        /* CPU whose scheduler is to be targetted. */
     uint32_t cmd;        /* XEN_SYSCTL_SCHEDOP_* */
     union {
         struct xen_sysctl_sched_arinc653 {
@@ -621,6 +631,9 @@
         } sched_arinc653;
         struct xen_sysctl_credit_schedule sched_credit;
         struct xen_sysctl_credit2_schedule sched_credit2;
+        struct xen_sysctl_sched_fp {
+            XEN_GUEST_HANDLE_64(xen_sysctl_fp_schedule_t) schedule;
+        } sched_fp;
     } u;
 };
 
diff -urBN xen/xen/include/xen/sched-if.h xen-4.10-rc5/xen/include/xen/sched-if.h
--- xen/xen/include/xen/sched-if.h	2017-12-15 14:52:52.889135000 +0100
+++ xen-4.10-rc5/xen/include/xen/sched-if.h	2017-11-21 17:10:01.847106000 +0100
@@ -12,6 +12,7 @@
 
 /* A global pointer to the initial cpupool (POOL0). */
 extern struct cpupool *cpupool0;
+extern struct cpupool *cpupool_list;
 
 /* cpus currently in no cpupool */
 extern cpumask_t cpupool_free_cpus;
diff -urBN xen/xen/include/xen/sched.h xen-4.10-rc5/xen/include/xen/sched.h
--- xen/xen/include/xen/sched.h	2017-12-15 14:52:52.889135000 +0100
+++ xen-4.10-rc5/xen/include/xen/sched.h	2017-11-21 17:10:01.847106000 +0100
@@ -765,6 +765,9 @@
        (_v) != NULL;                            \
        (_v) = (_v)->next_in_list )
 
+#define for_each_cpupool(ptr)    \
+    for ((ptr) = &cpupool_list; *(ptr) != NULL; (ptr) = &((*(ptr))->next))
+
 /*
  * Per-VCPU pause flags.
  */
